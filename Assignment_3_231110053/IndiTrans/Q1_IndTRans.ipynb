{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure to change path to the directory where you have cloned IndicTransTokenizer\n",
    "\n",
    "# %pip install accelerate\n",
    "# %pip install nltk\n",
    "# %pip install mosestokenizer\n",
    "# !pip install evaluate\n",
    "# !pip install rouge_score\n",
    "# % pip install  nltk\n",
    "# !git clone https://github.com/VarunGumma/IndicTransTokenizer\n",
    "# %cd IndicTransTokenizer\n",
    "# !pip install --editable /path_to/IndicTransTokenizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openRandomDataset(filepath):\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "        text = f.readlines()\n",
    "        text = [x.strip().replace('\\u200d', '') for x in text]\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Hindi,English and Marathi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hindi_1000 = openRandomDataset('../data/hindi_sentences.txt')\n",
    "eng_1000 = openRandomDataset('../data/eng_sentences.txt')\n",
    "mar_1000 = openRandomDataset('../data/marathi_sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_indic = \"ai4bharat/indictrans2-en-indic-dist-200M\"  # ai4bharat/indictrans2-en-indic-dist-200M\n",
    "indic_en = \"ai4bharat/indictrans2-indic-en-dist-200M\"  # ai4bharat/indictrans2-indic-en-dist-200M\n",
    "indic_indic = \"ai4bharat/indictrans2-indic-indic-dist-320M\"  # ai4bharat/indictrans2-indic-indic-dist-320M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading tokenizer , processor and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tusha\\anaconda3\\envs\\CS689\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IndicTransTokenizer import IndicTransTokenizer, IndicProcessor\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "def tokenizer_model_generate(address,direction):\n",
    "    tokenizer = IndicTransTokenizer(direction=direction)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        address,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    model.to(device)\n",
    "    return tokenizer,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation different tokenizer for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tusha\\anaconda3\\envs\\CS689\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "indi_en_tokenizer,indi_en_model = tokenizer_model_generate(indic_en,'indic-en')\n",
    "en_indi_tokenizer,en_indi_model = tokenizer_model_generate(en_indic,'en-indic')\n",
    "indi_indi_tokenizer,indi_indi_model = tokenizer_model_generate(indic_indic,'indic-indic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = IndicProcessor(inference=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_by_10(text_list,srclang,tgtlang,model,tokenizer):\n",
    "    batch_size = 10\n",
    "    num_batches = (len(text_list) + batch_size - 1) // batch_size\n",
    "    processed_texts = []\n",
    "    ip = IndicProcessor()\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min((i + 1) * batch_size, len(text_list))\n",
    "        batch_texts = text_list[batch_start:batch_end]\n",
    "\n",
    "        batch = ip.preprocess_batch(batch_texts, src_lang=srclang, tgt_lang=tgtlang)\n",
    "        batch = tokenizer(batch, src=True, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(**batch, num_beams=5, num_return_sequences=1, max_length=256)\n",
    "\n",
    "        outputs = tokenizer.batch_decode(outputs, src=False)\n",
    "        outputs = ip.postprocess_batch(outputs, lang=srclang)\n",
    "        processed_texts.extend(outputs)\n",
    "\n",
    "        # Free up memory if needed\n",
    "        del batch, outputs\n",
    "\n",
    "    return processed_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating the randomly selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1000 translations:  168.82941794395447\n",
      "Time taken for 1000 translations:  255.84121775627136\n",
      "Time taken for 1000 translations:  380.68901014328003\n",
      "Time taken for 1000 translations:  521.7725546360016\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "curr_time = time.time()\n",
    "translations_hi_eng = process_by_10(hindi_1000,srclang=\"hin_Deva\",tgtlang=\"eng_Latn\",model=indi_en_model,tokenizer=indi_en_tokenizer)\n",
    "print(\"Time taken for 1000 translations: \", time.time()-curr_time)\n",
    "curr_time = time.time()\n",
    "translations_eng_hi = process_by_10(eng_1000,srclang=\"eng_Latn\",tgtlang=\"hin_Deva\",model=en_indi_model,tokenizer=en_indi_tokenizer)\n",
    "print(\"Time taken for 1000 translations: \", time.time()-curr_time)\n",
    "curr_time = time.time()\n",
    "translations_mar_hi = process_by_10(mar_1000,srclang=\"mar_Deva\",tgtlang=\"hin_Deva\",model=indi_indi_model,tokenizer=indi_indi_tokenizer)\n",
    "print(\"Time taken for 1000 translations: \", time.time()-curr_time)\n",
    "curr_time = time.time()\n",
    "translations_hi_mar = process_by_10(hindi_1000,srclang=\"hin_Deva\",tgtlang=\"mar_Deva\",model=indi_indi_model,tokenizer=indi_indi_tokenizer)\n",
    "print(\"Time taken for 1000 translations: \", time.time()-curr_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving these files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../IndiTrans/indi_eng_hindi.txt\", \"w\",encoding='utf-8') as txt_file:\n",
    "    for line in translations_eng_hi:\n",
    "        txt_file.write(line+'\\n')\n",
    "with open(\"../IndiTrans/indi_hindi_eng.txt\", \"w\",encoding='utf-8') as txt_file:\n",
    "    for line in translations_hi_eng:\n",
    "        txt_file.write(line+'\\n')\n",
    "with open(\"../IndiTrans/indi_hindi_mar.txt\", \"w\",encoding='utf-8') as txt_file:\n",
    "    for line in translations_hi_mar:\n",
    "        txt_file.write(line+'\\n')\n",
    "with open(\"../IndiTrans/indi_mar_hindi.txt\", \"w\",encoding='utf-8') as txt_file:\n",
    "    for line in translations_mar_hi:\n",
    "        txt_file.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU and Rouge Score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marahti to Hindi\n",
      "BLEU: 0.275443319206238\n",
      "{'rouge1': 0.10458888888888888, 'rouge2': 0.022007142857142857, 'rougeL': 0.10369047619047622, 'rougeLsum': 0.10337976190476192}\n",
      "Hindi to Marathi\n",
      "BLEU: 0.20845461363921827\n",
      "{'rouge1': 0.08528809523809525, 'rouge2': 0.018333333333333333, 'rougeL': 0.08428214285714285, 'rougeLsum': 0.08398809523809524}\n",
      "Hindi to English\n",
      "BLEU: 0.4004007746985462\n",
      "{'rouge1': 0.7224173762505202, 'rouge2': 0.5128040839047556, 'rougeL': 0.676571144630105, 'rougeLsum': 0.6762574177693983}\n",
      "English to Hindi\n",
      "BLEU: 0.3284391709890271\n",
      "{'rouge1': 0.11264029304029305, 'rouge2': 0.022548484848484845, 'rougeL': 0.11104047619047622, 'rougeLsum': 0.11068690476190479}\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "def bleu_score(ref,pred):\n",
    "    score = 0\n",
    "    cc = SmoothingFunction()\n",
    "    for i in range(len(ref)):\n",
    "       references = ref[i].split()\n",
    "       candidate = pred[i].split() \n",
    "       score += sentence_bleu([references], candidate,smoothing_function=cc.method2)      \n",
    "    return score/len(ref)\n",
    "\n",
    "def rouge_score(ref,pred):\n",
    "    rougeS = evaluate.load('rouge')\n",
    "    rouge = rougeS.compute(references=ref, predictions=pred)\n",
    "    return rouge\n",
    "\n",
    "print(\"Marahti to Hindi\")\n",
    "print(\"BLEU:\",bleu_score(hindi_1000,translations_mar_hi))\n",
    "print(rouge_score(hindi_1000,translations_mar_hi))\n",
    "\n",
    "print(\"Hindi to Marathi\")\n",
    "print(\"BLEU:\",bleu_score(mar_1000,translations_hi_mar))\n",
    "print(rouge_score(mar_1000,translations_hi_mar))\n",
    "\n",
    "print(\"Hindi to English\")\n",
    "print(\"BLEU:\",bleu_score(eng_1000,translations_hi_eng))\n",
    "print(rouge_score(eng_1000,translations_hi_eng))\n",
    "\n",
    "print(\"English to Hindi\")\n",
    "print(\"BLEU:\",bleu_score(hindi_1000,translations_eng_hi))\n",
    "print(rouge_score(hindi_1000,translations_eng_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS689",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
