{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c10434b-3703-4cae-80ec-98589ca67c90",
   "metadata": {},
   "source": [
    "## Q1 In this question provide sentence inside val and run it will provide unicode correction and length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77805fa3-0dbf-4ed7-8985-24c1df48ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hi_100.txt', 'r', encoding='utf-8') as f:\n",
    "        corpus = f.readlines()\n",
    "        lines = corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2acd9a45-588a-41d3-92bd-58828319acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "consonants = {\n",
    "    \"क\": \"k\", \"ख\": \"kh\", \"ग\": \"g\", \"घ\": \"gh\", \"ङ\": \"N\",\n",
    "    \"च\": \"ch\", \"छ\": \"chh\", \"ज\": \"j\", \"झ\": \"jh\", \"ञ\": \"~N\",\n",
    "    \"ट\": \"T\", \"ठ\": \"Th\", \"ड\": \"D\", \"ढ\": \"Dh\", \"ण\": \"N\",\n",
    "    \"त\": \"t\", \"थ\": \"th\", \"द\": \"d\", \"ध\": \"dh\", \"न\": \"n\",\n",
    "    \"प\": \"p\", \"फ\": \"ph\", \"ब\": \"b\", \"भ\": \"bh\", \"म\": \"m\",\n",
    "    \"य\": \"y\", \"र\": \"r\", \"ल\": \"l\", \"व\": \"v\", \"श\": \"sh\",\n",
    "    \"ष\": \"Sh\", \"स\": \"s\", \"ह\": \"h\", \"ळ\": \"L\", \"क्ष\": \"kSh\",\n",
    "    \"ज्ञ\": \"j~n\", \"ड़\": \"R\", \"य़\": \"y\", \"ज़\": \"z\", \"ब़\": \"b\", \"क़\": \"q\", \"ख़\": \"Kh\", \"ग़\": \"G\",\n",
    "    \"ड़\": \"R\", \"ढ़\": \"Rh\", \"फ़\": \"f\", \"श़\": \"sh\", \"ऋ\": \"Ri\", \"ॠ\": \"Ri\", \"ॡ\": \"Li\", \"ऌ\": \"Li\", \"ऴ\": \"L\", \"ॐ\": \"OM\",\n",
    "    \"ऽ\": \"'\"\n",
    "         \"\"\n",
    "}\n",
    "hindi_to_english_transliteration = {\n",
    "    # Vowels\n",
    "    \"अ\": \"a\", \"आ\": \"aa\", \"इ\": \"i\", \"ई\": \"ii\", \"उ\": \"u\",\n",
    "    \"ऊ\": \"uu\", \"ऋ\": \"RRi\", \"ॠ\": \"RRi\", \"ऌ\": \"LLi\", \"ॡ\": \"LLi\",\n",
    "    \"ए\": \"e\", \"ऐ\": \"ai\", \"ओ\": \"o\", \"औ\": \"au\", \"अं\": \"am\", \"अः\": \"ah\",\n",
    "}\n",
    "matras = {\"ा\": \"आ\", \"ि\": \"इ\", \"ी\": \"ई\", \"ु\": \"उ\", \"ू\": \"ऊ\", \"ृ\": \"ऋ\", \"ॄ\": \"ॠ\", \"ॢ\": \"ऌ\", \"ॣ\": \"ॡ\", \"े\": \"ए\", \"ै\": \"ऐ\",\n",
    "          \"ो\": \"ओ\", \"ौ\": \"औ\", \"ं\": \"अं\", \"ः\": \"अः\", \"ँ\": \"अँ\"}\n",
    "reverse_matras = {\"अ\": \"\", \"आ\": \"ा\", \"इ\": \"ि\", \"ई\": \"ी\", \"उ\": \"ु\", \"ऊ\": \"ू\", \"ऋ\": \"ृ\", \"ॠ\": \"ॄ\", \"ऌ\": \"ॢ\", \"ॡ\": \"ॣ\",\n",
    "                  \"ए\": \"े\", \"ऐ\": \"ै\", \"ओ\": \"ो\", \"औ\": \"ौ\", \"अं\": \"ं\", \"अः\": \"ः\", \"अँ\": \"ँ\"}\n",
    "halanta = \"्\"\n",
    "halanta_char = []\n",
    "second_matras = {\"अं\": \"ं\", \"अः\": \"ः\", \"अँ\": \"ँ\"}\n",
    "reverse_matras_second = {\"ं\": \"अं\", \"ः\": \"अः\", \"ँ\": \"अँ\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b7482f-7852-43ff-b4ed-fee85f532aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_char = consonants.keys()\n",
    "for x in h_char:\n",
    "    halanta_char.append(x + halanta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced052af-4dbd-4321-81a6-5c773cec6ecc",
   "metadata": {},
   "source": [
    "#### Character Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a315b88-8378-40ce-9867-8e686746b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_conversion(line):\n",
    "    one = []\n",
    "    for lineIndex in range(len(line)):\n",
    "        if line[lineIndex] in consonants and lineIndex + 1 < len(line) and line[lineIndex + 1] != '्':\n",
    "            if line[lineIndex + 1] in matras:\n",
    "                one.append(line[lineIndex])\n",
    "                one.append(matras[line[lineIndex + 1]])\n",
    "                lineIndex += 1\n",
    "                if lineIndex+1<len(line) and line[lineIndex + 1] in matras:\n",
    "                    one.append(matras[line[lineIndex + 1]])\n",
    "                    lineIndex += 1\n",
    "            elif line[lineIndex + 1] == \"़\":\n",
    "                one.append(line[lineIndex] + \"़\")\n",
    "                if lineIndex + 2 < len(line) and line[lineIndex + 2] in second_matras:\n",
    "                    one.append(matras[line[lineIndex + 2]])\n",
    "                    lineIndex += 2\n",
    "                else:\n",
    "                    one.append('अ')\n",
    "            else:\n",
    "                one.append(line[lineIndex])\n",
    "                one.append('अ')\n",
    "        elif line[lineIndex] in consonants and lineIndex + 1 < len(line) and line[lineIndex + 1] == '्':\n",
    "            val = line[lineIndex]\n",
    "            val += \"्\"\n",
    "            one.append(val)\n",
    "            lineIndex += 1\n",
    "        elif line[lineIndex] in hindi_to_english_transliteration:\n",
    "            one.append(line[lineIndex])\n",
    "        # elif line[lineIndex] == \" \":\n",
    "        #     one.append(\" \")\n",
    "        else:\n",
    "            continue\n",
    "    return one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "593a134f-3ed8-4e32-aaef-d3e195542cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_halanta(val):\n",
    "    for x in range(len(val)-1):\n",
    "        if val[x] in consonants:\n",
    "            val[x] = val[x]+\"्\"\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7be8457-2c33-41e6-9347-d83602a3d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['त', 'उ', 'म्', 'ह', 'आ', 'र', 'आ', 'न', 'आ', 'म', 'अ', 'क्', 'य', 'आ', 'ह', 'ए']\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "val = character_conversion(\"तुम्हारा नाम क्या हे?\")\n",
    "print(char_to_halanta(val))\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c1edf-28ef-43b5-bf1e-08a6eb0aacc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e48f7f-fac9-46c8-bd7a-7268d85a8801",
   "metadata": {},
   "source": [
    "## Q2 Finding character and syllables unigram and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f911b9-dabd-4a3a-ac5a-704addf9eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topFreqUnigram(u_freq,k):\n",
    "    sorted_unigram_freq = sorted(u_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_k_unigram = dict(sorted_unigram_freq[:k])\n",
    "    for items in top_k_unigram:\n",
    "        print(f\"{items}: {top_k_unigram[items]}\")\n",
    "    return top_k_unigram\n",
    "\n",
    "def topFreqBigram(b_freq,k):\n",
    "    sorted_bigram_freq = sorted(b_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_k_bigram = dict(sorted_bigram_freq[:k])\n",
    "    for items in top_k_bigram:\n",
    "        print(f\"{items}: {top_k_bigram[items]}\")\n",
    "    return top_k_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1167b-0069-47bc-8cd8-23fc0dfe75e6",
   "metadata": {},
   "source": [
    "#### Syllable Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e727113f-0b09-49ab-bdcb-d5b856d0d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_convert(val):\n",
    "    a = \"\"\n",
    "    one = []\n",
    "    i = 0\n",
    "    while i < len(val):\n",
    "        # if val[i] == \" \":\n",
    "        #     # one.append(\" \")\n",
    "        #     i += 1\n",
    "        #     continue\n",
    "        # print(i)\n",
    "        if val[i] in consonants:\n",
    "            a = a + val[i]\n",
    "        elif val[i] in halanta_char:\n",
    "            a = a + val[i]\n",
    "        else:\n",
    "            if a != \"\":\n",
    "                if val[i] in reverse_matras:\n",
    "                    a = a + reverse_matras[val[i]]\n",
    "                if i + 1 < len(val) and val[i + 1] in second_matras:\n",
    "                    a = a + second_matras[val[i + 1]]\n",
    "                    i = i + 1\n",
    "            else:\n",
    "                a = val[i]\n",
    "            one.append(a)\n",
    "            a = \"\"\n",
    "        i += 1\n",
    "        # else:\n",
    "        #     one.append(val[i])\n",
    "    for x in one:\n",
    "        if x == \"\":\n",
    "            one.remove(x)\n",
    "    return one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e459e44-d47b-495d-b463-68331a350c52",
   "metadata": {},
   "source": [
    "#### Finding Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27f70e0d-efc8-4bb7-8062-57de754c7504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllable Unigram Frequency:\n",
      "र: 1007332\n",
      "क: 604341\n",
      "न: 504714\n",
      "स: 489101\n",
      "के: 397246\n",
      "प: 391011\n",
      "ल: 328365\n",
      "ने: 325175\n",
      "त: 303783\n",
      "का: 294287\n",
      "म: 293458\n",
      "ए: 287987\n",
      "ह: 278528\n",
      "में: 259063\n",
      "अ: 253328\n",
      "ब: 246861\n",
      "की: 234951\n",
      "ग: 225107\n",
      "है: 216182\n",
      "या: 213712\n",
      "Syllable Bigram Frequency:\n",
      "क र: 161771\n",
      "औ र: 115585\n",
      "प र: 99975\n",
      "इ स: 83107\n",
      "ए क: 59487\n",
      "लि ए: 54067\n",
      "के लि: 50098\n",
      "न हीं: 47399\n",
      "अ प: 45193\n",
      "र ने: 44066\n",
      "त क: 40316\n",
      "का र: 39821\n",
      "कि या: 37105\n",
      "ता है: 36522\n",
      "ने के: 35282\n",
      "स के: 34216\n",
      "न के: 32861\n",
      "क हा: 32812\n",
      "र का: 32562\n",
      "य ह: 31774\n",
      "Character Unigram Frequency:\n",
      "अ: 7153322\n",
      "आ: 2966629\n",
      "ए: 2303822\n",
      "क: 2059392\n",
      "र: 1885114\n",
      "ई: 1445367\n",
      "इ: 1425781\n",
      "न: 1250677\n",
      "ह: 1130489\n",
      "स: 1115612\n",
      "म: 1022283\n",
      "ओ: 891886\n",
      "ल: 862535\n",
      "त: 852908\n",
      "य: 752198\n",
      "प: 664385\n",
      "व: 604494\n",
      "उ: 586139\n",
      "द: 546709\n",
      "ज: 514198\n",
      "Character Bigram Frequency:\n",
      "र अ: 1173240\n",
      "अ क: 921468\n",
      "अ र: 786891\n",
      "क अ: 616852\n",
      "अ ह: 562925\n",
      "अ न: 551313\n",
      "स अ: 518739\n",
      "न अ: 515816\n",
      "अ म: 467030\n",
      "क ए: 407129\n",
      "प अ: 405636\n",
      "त अ: 353263\n",
      "अ स: 348242\n",
      "आ र: 343947\n",
      "ए अं: 341741\n",
      "ल अ: 333296\n",
      "अ त: 332006\n",
      "न ए: 328953\n",
      "म अ: 324890\n",
      "क आ: 314321\n",
      "CPU times: total: 1min 10s\n",
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'र अ': 1173240,\n",
       " 'अ क': 921468,\n",
       " 'अ र': 786891,\n",
       " 'क अ': 616852,\n",
       " 'अ ह': 562925,\n",
       " 'अ न': 551313,\n",
       " 'स अ': 518739,\n",
       " 'न अ': 515816,\n",
       " 'अ म': 467030,\n",
       " 'क ए': 407129,\n",
       " 'प अ': 405636,\n",
       " 'त अ': 353263,\n",
       " 'अ स': 348242,\n",
       " 'आ र': 343947,\n",
       " 'ए अं': 341741,\n",
       " 'ल अ': 333296,\n",
       " 'अ त': 332006,\n",
       " 'न ए': 328953,\n",
       " 'म अ': 324890,\n",
       " 'क आ': 314321}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "unigram_freq = {}\n",
    "unigram_freq_char = {}\n",
    "bigram_freq = {}\n",
    "bigram_freq_char = {}\n",
    "x = 0\n",
    "for line in lines:\n",
    "    val = character_conversion(line)\n",
    "    freq = syllable_convert(val)\n",
    "    for it in val:\n",
    "        if it in unigram_freq:\n",
    "            unigram_freq_char[it] += 1\n",
    "        else:\n",
    "            unigram_freq_char[it] = 1\n",
    "    for item in freq:\n",
    "        if item in unigram_freq:\n",
    "            unigram_freq[item] += 1\n",
    "        else:\n",
    "            unigram_freq[item] = 1\n",
    "\n",
    "    for it1 in range(len(val) - 1):\n",
    "        bigram = \" \".join((val[it1], val[it1 + 1]))\n",
    "        if bigram in bigram_freq_char:\n",
    "            bigram_freq_char[bigram] += 1\n",
    "        else:\n",
    "            bigram_freq_char[bigram] = 1\n",
    "    for i in range(len(freq) - 1):\n",
    "        bigram = \" \".join((freq[i], freq[i + 1]))\n",
    "        if bigram in bigram_freq:\n",
    "            bigram_freq[bigram] += 1\n",
    "        else:\n",
    "            bigram_freq[bigram] = 1\n",
    "# # top 20 values\n",
    "print(\"Syllable Unigram Frequency:\")\n",
    "topFreqUnigram(unigram_freq,20)\n",
    "print(\"Syllable Bigram Frequency:\")\n",
    "topFreqBigram(bigram_freq,20)\n",
    "print(\"Character Unigram Frequency:\")\n",
    "topFreqUnigram(unigram_freq_char,20)\n",
    "print(\"Character Bigram Frequency:\")\n",
    "topFreqBigram(bigram_freq_char,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b8a98-d484-40f8-846c-f1f6fd5e50f1",
   "metadata": {},
   "source": [
    "## Q4 Finding unigram frequency of token and bigram frequencies of tokens, syllables and characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d61b4-fa1e-4478-96f2-4e63d1eb753a",
   "metadata": {},
   "source": [
    "#### Loading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc02336f-2f57-4c52-8dcf-976ce85f0818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आवेदन करने की आखिरी तारीख 31 जनवरी, 2020 है।\n",
      "इतनी दुआ कर दो हमारे लिए कि जितना प्यार दुनिया ने आपको दिया है, बस उतना ही हमें भी मिल जाए|”\n",
      "मोदी सरकार के पहले कार्यकाल में भी तीन तलाक को लेकर बिल लाया गया था, हालांकि तब यह राज्यसभा में पास नहीं हो पाया था.\n",
      "भाजपा के दिवंगत नेता प्रमोद महाजन की बेटी पूनम महाजन को सचिव बनाया गया है.\n",
      "ऐसी स्थिति में एक न्यायपूर्ण सरकार सार्वजनिक वित्त का इस तरह इस्तेमाल करती है कि संसाधनों का आवंटन, सभी के उपभोग वाले उत्पादों की व्यवहार्यता और समग्र वृहद-आर्थिक प्रबंधन 'निष्पक्षता के रूप में न्याय' को बढ़ाए।\n",
      "दिलचस्प है कि डीसीएचएल के चेयरमैन टी वेंकटरमन रेड्डी और वाइस चेयरमैन टी विनायक रवि रेड्डी इस बैठक में मौजूद नहीं थे।\n",
      "इस आम चुनाव में भाजपा नेता सतीश कुमार गौतम को सबसे अधिक 6 लाख 56 हजार 215 वोट प्राप्त हुए.\n",
      "आयरलैंड टीम के विकेटकीपर बल्लेबाज नियाल ओ'ब्रायन ने अंतर्राष्ट्रीय क्रिकेट से संन्यास का ऐलान कर दिया है। साल 2002 में डेनमार्क के खिलाफ के एकदिवसीय क्रिकेट में डेब्यू करने वाले ब्रायन ने 36 साल की उम्र में संन्यास का ऐलान किया। नियाल ओ'ब्रायन आयरलैंड\n"
     ]
    }
   ],
   "source": [
    "file_path = 'hi_100.txt'  \n",
    "try:\n",
    "    with open(file_path, 'r',encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "        print(data[:1000])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def countF(tokenizer,data):\n",
    "    tokens = tokenizer.encode(data, out_type=str)\n",
    "    unigram_frequencies = {}\n",
    "    for token in tokens:\n",
    "        if token in unigram_frequencies:\n",
    "            unigram_frequencies[token] += 1\n",
    "        else:\n",
    "            unigram_frequencies[token] = 1\n",
    "    bigrams = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "    bigram_frequencies = {}\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_frequencies:\n",
    "            bigram_frequencies[bigram] += 1\n",
    "        else:\n",
    "            bigram_frequencies[bigram] = 1\n",
    "    syllables = []\n",
    "    characters = []\n",
    "    for tok in tokens:\n",
    "        syllables.extend(syllable_convert(tok))\n",
    "        characters.extend(character_conversion(tok))\n",
    "    bigram_syllables = [(syllables[i], syllables[i + 1]) for i in range(len(syllables) - 1)]\n",
    "    bigram_characters = [(characters[i], characters[i + 1]) for i in range(len(characters) - 1)]\n",
    "    syllable_freqs = Counter(bigram_syllables)\n",
    "    char_freqs = Counter(bigram_characters)\n",
    "    return unigram_frequencies, bigram_frequencies, syllable_freqs, char_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1741cb-72e9-470e-bcc3-2329cc3603c6",
   "metadata": {},
   "source": [
    "#### BPE and Unigram Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7b7d09-11fb-42df-a4c6-9aeeed34e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def unigram_tokenizer(file_path, v_size):\n",
    "    spm.SentencePieceTrainer.Train(input=file_path, model_prefix='uni_model'+str(v_size),model_type='unigram', vocab_size=v_size)\n",
    "    model_file='uni_model'+str(v_size)+'.model'\n",
    "    tokenizer_unigram = spm.SentencePieceProcessor('uni_model'+str(v_size)+'.model')\n",
    "    return tokenizer_unigram\n",
    "\n",
    "def bpe_tokenizer(file_path, v_size):\n",
    "    spm.SentencePieceTrainer.train(input=file_path, model_prefix='bpe_model'+str(v_size), model_type='bpe', vocab_size=v_size)\n",
    "    bpe_tknizer = spm.SentencePieceProcessor('bpe_model'+str(v_size)+'.model')\n",
    "    return bpe_tknizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b7c86-d38f-48f0-93fc-fc514f56000a",
   "metadata": {},
   "source": [
    "#### Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed367db-f720-4559-994f-41d25904db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for writing files\n",
    "def model_save(model_name,v_size,arr):\n",
    "    uni,bi,syll,char = arr \n",
    "    s_uni = sorted(uni.items(), key=lambda x: x[1], reverse=True)\n",
    "    file_path = \"\"\n",
    "    file_name = model_name+'_unigram_tokens_'+str(v_size)+'.txt'\n",
    "    with open(file_path+file_name, 'w', encoding='utf-8') as f:\n",
    "        for word, count in s_uni:\n",
    "            f.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "    s_bi = sorted(bi.items(), key=lambda x: x[1], reverse=True)\n",
    "    file_name = model_name + \"_bigram_tokens_\" +str(v_size)+ \".txt\"\n",
    "    with open(file_path+file_name, 'w', encoding='utf-8') as f:\n",
    "        for (word1, word2), count in s_bi:\n",
    "            f.write(f\"{word1} {word2}: {count}\\n\")\n",
    "\n",
    "    s_syll = sorted(syll.items(), key=lambda x: x[1], reverse=True)\n",
    "    file_name = model_name + \"_bi_syllables_\" +str(v_size)+ \".txt\"\n",
    "    with open(file_path+file_name, 'w', encoding='utf-8') as f:\n",
    "        for (word1, word2), count in s_syll:\n",
    "            f.write(f\"{word1} {word2}: {count}\\n\")\n",
    "    s_char = sorted(char.items(), key=lambda x: x[1], reverse=True)\n",
    "    file_name = model_name + \"_bi_char_\" +str(v_size)+ \".txt\"\n",
    "    with open(file_path+file_name, 'w', encoding='utf-8') as f:\n",
    "        for (word1, word2), count in s_char:\n",
    "            f.write(f\"{word1} {word2}: {count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbccd68-71a9-439c-a6ef-481d3f741a86",
   "metadata": {},
   "source": [
    "#### Unigram_1000 vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf967051-6887-417c-9185-c449889e8e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uni\n",
      "र: 431118\n",
      "▁के: 328920\n",
      "न: 281629\n",
      "।: 276966\n",
      "ा: 256523\n",
      "ल: 244768\n",
      "▁में: 240292\n",
      "▁: 233755\n",
      "▁है: 215655\n",
      "ी: 211097\n",
      "▁की: 199499\n",
      ",: 191476\n",
      "म: 185049\n",
      "े: 184611\n",
      "▁को: 159954\n",
      "▁से: 154736\n",
      "क: 154277\n",
      "त: 145275\n",
      "ो: 137056\n",
      "ि: 134016\n",
      "bi\n",
      "('▁है', '।'): 89363\n",
      "('▁के', '▁लिए'): 43425\n",
      "('▁हैं', '।'): 33632\n",
      "('▁है', '.'): 25838\n",
      "('▁है', '▁कि'): 25125\n",
      "('ों', '▁के'): 21724\n",
      "('र', 'ा'): 19993\n",
      "('ों', '▁में'): 19869\n",
      "('▁है', ','): 19551\n",
      "('▁के', '▁साथ'): 18346\n",
      "('र', 'ी'): 17879\n",
      "('ते', '▁हैं'): 17736\n",
      "('ा', 'र'): 17496\n",
      "('ने', '▁के'): 17090\n",
      "('ों', '▁को'): 16905\n",
      "('ता', '▁है'): 16224\n",
      "('▁कहा', '▁कि'): 16099\n",
      "('।', '▁इस'): 14551\n",
      "('▁के', '▁बाद'): 14451\n",
      "('ों', '▁की'): 14109\n",
      "syllable\n",
      "('▁', 'क'): 1164117\n",
      "('क', '▁'): 1059456\n",
      "('ं', '▁'): 752803\n",
      "('▁', 'ह'): 522370\n",
      "('▁', 'म'): 464845\n",
      "('▁', 'स'): 452985\n",
      "('▁', '▁'): 418607\n",
      "('न', '▁'): 393052\n",
      "('स', '▁'): 325851\n",
      "('त', '▁'): 284825\n",
      "('र', '▁'): 279269\n",
      "('।', '▁'): 272992\n",
      "('म', 'ं'): 264559\n",
      "('▁', 'अ'): 245641\n",
      "('▁', 'प'): 241973\n",
      "('▁', 'न'): 236433\n",
      "('▁', 'द'): 209848\n",
      "('य', '▁'): 205941\n",
      "('ल', '▁'): 194507\n",
      "('▁', 'ज'): 192391\n",
      "char\n",
      "('क', 'ए'): 400441\n",
      "('आ', 'क'): 397593\n",
      "('न', 'ए'): 323222\n",
      "('क', 'आ'): 300427\n",
      "('ह', 'ऐ'): 296118\n",
      "('क', 'अ'): 292726\n",
      "('य', 'आ'): 288660\n",
      "('ए', 'क'): 274585\n",
      "('अ', 'ह'): 272906\n",
      "('ए', 'अं'): 270356\n",
      "('र', 'अ'): 267824\n",
      "('म', 'ए'): 267626\n",
      "('अ', 'क'): 263891\n",
      "('प', 'अ'): 248206\n",
      "('क', 'ई'): 232707\n",
      "('अ', 'न'): 208993\n",
      "('स', 'ए'): 207418\n",
      "('आ', 'ह'): 206676\n",
      "('क', 'ओ'): 198975\n",
      "('स', 'अ'): 196480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('क', 'ए'): 400441,\n",
       " ('आ', 'क'): 397593,\n",
       " ('न', 'ए'): 323222,\n",
       " ('क', 'आ'): 300427,\n",
       " ('ह', 'ऐ'): 296118,\n",
       " ('क', 'अ'): 292726,\n",
       " ('य', 'आ'): 288660,\n",
       " ('ए', 'क'): 274585,\n",
       " ('अ', 'ह'): 272906,\n",
       " ('ए', 'अं'): 270356,\n",
       " ('र', 'अ'): 267824,\n",
       " ('म', 'ए'): 267626,\n",
       " ('अ', 'क'): 263891,\n",
       " ('प', 'अ'): 248206,\n",
       " ('क', 'ई'): 232707,\n",
       " ('अ', 'न'): 208993,\n",
       " ('स', 'ए'): 207418,\n",
       " ('आ', 'ह'): 206676,\n",
       " ('क', 'ओ'): 198975,\n",
       " ('स', 'अ'): 196480}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_1000_tokenizer = unigram_tokenizer('hi_100.txt', 1000)\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = countF(unigram_1000_tokenizer, data)\n",
    "arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "model_save(model_name=\"unigram\",v_size=1000,arr=arr)\n",
    "print(\"uni\")\n",
    "topFreqUnigram(unigram_freqs,20)\n",
    "print(\"bi\")\n",
    "topFreqUnigram(bigram_freqs,20)\n",
    "print(\"syllable\")\n",
    "topFreqUnigram(syllable_freqs,20)\n",
    "print(\"char\")\n",
    "topFreqUnigram(char_freqs,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b352d-de0a-426a-9356-097a9bbbeb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afb42cee-6ee4-4cf9-82ab-db90de6fc6bd",
   "metadata": {},
   "source": [
    "#### Unigram_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1b1ebb7-7b00-409a-8baf-f0d136f8ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁के: 324723\n",
      "।: 273666\n",
      "▁में: 240292\n",
      "▁है: 215632\n",
      "▁की: 196738\n",
      ",: 190026\n",
      "ी: 173237\n",
      "▁को: 154736\n",
      "▁से: 151426\n",
      "ल: 146463\n",
      "र: 145878\n",
      "न: 142629\n",
      "▁का: 131131\n",
      "▁: 126257\n",
      ".: 121201\n",
      "ा: 117568\n",
      "▁और: 115376\n",
      "क: 110608\n",
      "ों: 109310\n",
      "▁ने: 107299\n",
      "('▁है', '।'): 89363\n",
      "('▁के', '▁लिए'): 43425\n",
      "('▁हैं', '।'): 33632\n",
      "('▁है', '.'): 25841\n",
      "('▁है', '▁कि'): 25121\n",
      "('▁है', ','): 19551\n",
      "('ों', '▁के'): 19271\n",
      "('▁के', '▁साथ'): 18346\n",
      "('ों', '▁में'): 17781\n",
      "('▁कहा', '▁कि'): 16097\n",
      "('ों', '▁को'): 14835\n",
      "('▁के', '▁बाद'): 14451\n",
      "('▁था', '।'): 12626\n",
      "('ों', '▁की'): 12609\n",
      "('▁रहा', '▁है'): 12021\n",
      "('ने', '▁के'): 11985\n",
      "('▁ने', '▁कहा'): 11830\n",
      "('▁हैं', '.'): 11093\n",
      "('▁गया', '▁है'): 11051\n",
      "('।', '▁इस'): 10846\n",
      "('▁', 'क'): 1153274\n",
      "('क', '▁'): 1042132\n",
      "('ं', '▁'): 738867\n",
      "('▁', 'ह'): 524734\n",
      "('▁', 'म'): 469974\n",
      "('▁', 'स'): 459403\n",
      "('▁', '▁'): 418607\n",
      "('न', '▁'): 375091\n",
      "('स', '▁'): 330014\n",
      "('र', '▁'): 314242\n",
      "('।', '▁'): 272992\n",
      "('म', 'ं'): 269732\n",
      "('त', '▁'): 269345\n",
      "('▁', 'प'): 263727\n",
      "('▁', 'अ'): 244256\n",
      "('▁', 'न'): 230018\n",
      "('▁', 'द'): 220728\n",
      "('य', '▁'): 213129\n",
      "('▁', 'ज'): 210779\n",
      "('ल', '▁'): 193440\n",
      "('क', 'ए'): 399726\n",
      "('आ', 'क'): 383301\n",
      "('क', 'अ'): 336247\n",
      "('र', 'अ'): 333141\n",
      "('न', 'ए'): 321740\n",
      "('अ', 'क'): 316580\n",
      "('अ', 'ह'): 310675\n",
      "('क', 'आ'): 307132\n",
      "('ह', 'ऐ'): 296084\n",
      "('य', 'आ'): 291860\n",
      "('ए', 'अं'): 291735\n",
      "('म', 'ए'): 290675\n",
      "('प', 'अ'): 287430\n",
      "('ए', 'क'): 272411\n",
      "('अ', 'र'): 253669\n",
      "('स', 'अ'): 246782\n",
      "('अ', 'न'): 238023\n",
      "('क', 'ई'): 231881\n",
      "('आ', 'ह'): 215208\n",
      "('स', 'ए'): 212515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('क', 'ए'): 399726,\n",
       " ('आ', 'क'): 383301,\n",
       " ('क', 'अ'): 336247,\n",
       " ('र', 'अ'): 333141,\n",
       " ('न', 'ए'): 321740,\n",
       " ('अ', 'क'): 316580,\n",
       " ('अ', 'ह'): 310675,\n",
       " ('क', 'आ'): 307132,\n",
       " ('ह', 'ऐ'): 296084,\n",
       " ('य', 'आ'): 291860,\n",
       " ('ए', 'अं'): 291735,\n",
       " ('म', 'ए'): 290675,\n",
       " ('प', 'अ'): 287430,\n",
       " ('ए', 'क'): 272411,\n",
       " ('अ', 'र'): 253669,\n",
       " ('स', 'अ'): 246782,\n",
       " ('अ', 'न'): 238023,\n",
       " ('क', 'ई'): 231881,\n",
       " ('आ', 'ह'): 215208,\n",
       " ('स', 'ए'): 212515}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_2000_tokenizer = unigram_tokenizer('hi_100.txt', 2000)\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = countF(unigram_2000_tokenizer, data)\n",
    "arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "model_save(model_name=\"unigram\",v_size=2000,arr=arr)\n",
    "topFreqUnigram(unigram_freqs,20)\n",
    "topFreqUnigram(bigram_freqs,20)\n",
    "topFreqUnigram(syllable_freqs,20)\n",
    "topFreqUnigram(char_freqs,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07455351-b61d-4111-9b54-9699dc6ba324",
   "metadata": {},
   "source": [
    "#### BPE_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a80f9cc8-fba3-4270-a23c-68eed25bb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁के: 328806\n",
      "।: 270047\n",
      "▁में: 240292\n",
      "▁है: 215519\n",
      "▁की: 199499\n",
      ",: 186960\n",
      "▁को: 162462\n",
      "ल: 158685\n",
      "न: 156906\n",
      "▁से: 152622\n",
      "क: 144398\n",
      "त: 140020\n",
      "▁: 133468\n",
      "▁का: 123398\n",
      "म: 123257\n",
      "र: 121598\n",
      ".: 116875\n",
      "▁और: 115365\n",
      "े: 112549\n",
      "▁ने: 110224\n",
      "('▁है', '।'): 89363\n",
      "('▁के', '▁लिए'): 43425\n",
      "('▁हैं', '।'): 33632\n",
      "('▁है', '.'): 25579\n",
      "('▁है', '▁कि'): 25130\n",
      "('▁है', ','): 19551\n",
      "('▁के', '▁साथ'): 18346\n",
      "('ों', '▁के'): 17977\n",
      "('ने', '▁के'): 16651\n",
      "('▁कहा', '▁कि'): 16100\n",
      "('ों', '▁में'): 16067\n",
      "('ते', '▁हैं'): 15852\n",
      "('▁के', '▁बाद'): 14451\n",
      "('ों', '▁को'): 14254\n",
      "('ता', '▁है'): 13198\n",
      "('।', '▁इस'): 12725\n",
      "('▁था', '।'): 12626\n",
      "('▁रहा', '▁है'): 12021\n",
      "('▁ने', '▁कहा'): 11824\n",
      "('ों', '▁की'): 11499\n",
      "('▁', 'क'): 1146933\n",
      "('क', '▁'): 1070351\n",
      "('ं', '▁'): 751497\n",
      "('▁', 'ह'): 536002\n",
      "('▁', 'स'): 454872\n",
      "('▁', 'म'): 429313\n",
      "('▁', '▁'): 418607\n",
      "('न', '▁'): 406735\n",
      "('र', '▁'): 347487\n",
      "('स', '▁'): 342313\n",
      "('त', '▁'): 303380\n",
      "('।', '▁'): 272992\n",
      "('म', 'ं'): 272813\n",
      "('▁', 'अ'): 245641\n",
      "('▁', 'न'): 233158\n",
      "('▁', 'प'): 220766\n",
      "('य', '▁'): 216625\n",
      "('ल', '▁'): 207526\n",
      "('▁', 'ा'): 201834\n",
      "('▁', 'द'): 199665\n",
      "('क', 'ए'): 393586\n",
      "('आ', 'क'): 386568\n",
      "('न', 'ए'): 325957\n",
      "('क', 'अ'): 304719\n",
      "('ह', 'ऐ'): 296118\n",
      "('य', 'आ'): 288185\n",
      "('क', 'आ'): 287318\n",
      "('म', 'ए'): 274562\n",
      "('ए', 'क'): 273311\n",
      "('ए', 'अं'): 271900\n",
      "('अ', 'क'): 258782\n",
      "('अ', 'ह'): 242105\n",
      "('क', 'ई'): 234396\n",
      "('र', 'अ'): 233647\n",
      "('प', 'अ'): 231507\n",
      "('स', 'ए'): 212670\n",
      "('अ', 'न'): 210328\n",
      "('क', 'ओ'): 202105\n",
      "('आ', 'ह'): 200429\n",
      "('ई', 'क'): 194279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('क', 'ए'): 393586,\n",
       " ('आ', 'क'): 386568,\n",
       " ('न', 'ए'): 325957,\n",
       " ('क', 'अ'): 304719,\n",
       " ('ह', 'ऐ'): 296118,\n",
       " ('य', 'आ'): 288185,\n",
       " ('क', 'आ'): 287318,\n",
       " ('म', 'ए'): 274562,\n",
       " ('ए', 'क'): 273311,\n",
       " ('ए', 'अं'): 271900,\n",
       " ('अ', 'क'): 258782,\n",
       " ('अ', 'ह'): 242105,\n",
       " ('क', 'ई'): 234396,\n",
       " ('र', 'अ'): 233647,\n",
       " ('प', 'अ'): 231507,\n",
       " ('स', 'ए'): 212670,\n",
       " ('अ', 'न'): 210328,\n",
       " ('क', 'ओ'): 202105,\n",
       " ('आ', 'ह'): 200429,\n",
       " ('ई', 'क'): 194279}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_1000_tokenizer = bpe_tokenizer('hi_100.txt', 1000)\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = countF(bpe_1000_tokenizer, data)\n",
    "arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "model_save(model_name=\"bpe\",v_size=1000,arr=arr)\n",
    "topFreqUnigram(unigram_freqs,20)\n",
    "topFreqUnigram(bigram_freqs,20)\n",
    "topFreqUnigram(syllable_freqs,20)\n",
    "topFreqUnigram(char_freqs,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d90cf6-b44c-43c5-9f4a-8baf8be2c3bf",
   "metadata": {},
   "source": [
    "#### BPE_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f2b5278-21cb-4592-8f42-6123a7f7ce93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁के: 326040\n",
      "।: 270047\n",
      "▁में: 240292\n",
      "▁है: 215519\n",
      "▁की: 197474\n",
      ",: 185205\n",
      "▁को: 156151\n",
      "▁से: 147346\n",
      "▁का: 123398\n",
      ".: 116875\n",
      "▁और: 115365\n",
      "▁ने: 108464\n",
      "▁पर: 100405\n",
      "क: 90482\n",
      "▁कि: 89667\n",
      "न: 82236\n",
      "▁हैं: 80599\n",
      "ों: 77433\n",
      "ल: 73155\n",
      "म: 71367\n",
      "('▁है', '।'): 89363\n",
      "('▁के', '▁लिए'): 43425\n",
      "('▁हैं', '।'): 33632\n",
      "('▁है', '.'): 25579\n",
      "('▁है', '▁कि'): 25108\n",
      "('▁है', ','): 19551\n",
      "('▁के', '▁साथ'): 18346\n",
      "('▁कहा', '▁कि'): 16096\n",
      "('▁के', '▁बाद'): 14451\n",
      "('ों', '▁के'): 13475\n",
      "('ों', '▁में'): 13042\n",
      "('▁था', '।'): 12626\n",
      "('▁रहा', '▁है'): 12021\n",
      "('▁ने', '▁कहा'): 11824\n",
      "('ने', '▁के'): 11590\n",
      "('ता', '▁है'): 11120\n",
      "('▁गया', '▁है'): 11051\n",
      "('▁हैं', '.'): 10997\n",
      "('।', '▁इस'): 10693\n",
      "('▁रहे', '▁हैं'): 10180\n",
      "('▁', 'क'): 1157614\n",
      "('क', '▁'): 1055357\n",
      "('ं', '▁'): 743692\n",
      "('▁', 'ह'): 524524\n",
      "('▁', 'म'): 471201\n",
      "('▁', 'स'): 457869\n",
      "('▁', '▁'): 418607\n",
      "('न', '▁'): 389912\n",
      "('स', '▁'): 342860\n",
      "('र', '▁'): 339484\n",
      "('त', '▁'): 291970\n",
      "('।', '▁'): 272992\n",
      "('म', 'ं'): 271581\n",
      "('▁', 'प'): 253433\n",
      "('▁', 'अ'): 245641\n",
      "('▁', 'न'): 233909\n",
      "('▁', 'द'): 217402\n",
      "('य', '▁'): 216467\n",
      "('▁', 'ज'): 210396\n",
      "('ल', '▁'): 203960\n",
      "('क', 'ए'): 402872\n",
      "('आ', 'क'): 377294\n",
      "('क', 'अ'): 330433\n",
      "('अ', 'क'): 326235\n",
      "('न', 'ए'): 325957\n",
      "('अ', 'ह'): 304979\n",
      "('क', 'आ'): 304314\n",
      "('ह', 'ऐ'): 296118\n",
      "('ए', 'अं'): 294293\n",
      "('र', 'अ'): 293990\n",
      "('य', 'आ'): 291608\n",
      "('म', 'ए'): 285742\n",
      "('प', 'अ'): 284357\n",
      "('ए', 'क'): 275062\n",
      "('अ', 'र'): 252010\n",
      "('स', 'अ'): 246606\n",
      "('अ', 'न'): 245780\n",
      "('क', 'ई'): 236594\n",
      "('स', 'ए'): 212670\n",
      "('आ', 'ह'): 209328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('क', 'ए'): 402872,\n",
       " ('आ', 'क'): 377294,\n",
       " ('क', 'अ'): 330433,\n",
       " ('अ', 'क'): 326235,\n",
       " ('न', 'ए'): 325957,\n",
       " ('अ', 'ह'): 304979,\n",
       " ('क', 'आ'): 304314,\n",
       " ('ह', 'ऐ'): 296118,\n",
       " ('ए', 'अं'): 294293,\n",
       " ('र', 'अ'): 293990,\n",
       " ('य', 'आ'): 291608,\n",
       " ('म', 'ए'): 285742,\n",
       " ('प', 'अ'): 284357,\n",
       " ('ए', 'क'): 275062,\n",
       " ('अ', 'र'): 252010,\n",
       " ('स', 'अ'): 246606,\n",
       " ('अ', 'न'): 245780,\n",
       " ('क', 'ई'): 236594,\n",
       " ('स', 'ए'): 212670,\n",
       " ('आ', 'ह'): 209328}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_2000_tokenizer = bpe_tokenizer('hi_100.txt', 2000)\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = countF(bpe_2000_tokenizer, data)\n",
    "arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "model_save(model_name=\"bpe\",v_size=2000,arr=arr)\n",
    "topFreqUnigram(unigram_freqs,20)\n",
    "topFreqUnigram(bigram_freqs,20)\n",
    "topFreqUnigram(syllable_freqs,20)\n",
    "topFreqUnigram(char_freqs,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166591f7-1165-4589-9024-e39ea2b5806b",
   "metadata": {},
   "source": [
    "#### BERT 1000 & 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84c917d0-c838-4f12-a13f-5abf80cdb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import time\n",
    "def mBert():\n",
    "    bertToken = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    return bertToken\n",
    "\n",
    "def countF1(tokens):\n",
    "    u_freq = Counter(tokens)\n",
    "    bigrams = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "    bigram_frequencies = {}\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_frequencies:\n",
    "            bigram_frequencies[bigram] += 1\n",
    "        else:\n",
    "            bigram_frequencies[bigram] = 1\n",
    "    syllables = []\n",
    "    char = []\n",
    "    for tkn in tokens:\n",
    "        syllables.extend(syllable_convert(tkn))\n",
    "        char.extend(character_conversion(tkn))\n",
    "    bi_syllables = [(syllables[i], syllables[i + 1]) for i in range(len(syllables) - 1)]\n",
    "    bi_characters = [(char[i], char[i + 1]) for i in range(len(char) - 1)]\n",
    "    syllable_freqs = Counter(bi_syllables)\n",
    "    char_freqs = Counter(bi_characters)\n",
    "    return u_freq, bigram_frequencies, syllable_freqs, char_freqs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce5dc20b-a588-491e-b3de-83d855be134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "के: 22\n",
      "में: 20\n",
      "।: 18\n",
      "है: 15\n",
      "ब: 15\n",
      "##प: 14\n",
      "आ: 13\n",
      ",: 13\n",
      "को: 13\n",
      "##ी: 12\n",
      "##र: 12\n",
      "और: 12\n",
      "##ा: 11\n",
      "ख: 11\n",
      "##न: 10\n",
      "की: 9\n",
      "##स: 8\n",
      "म: 8\n",
      "स: 8\n",
      "का: 8\n",
      "हैं: 8\n",
      "कर: 7\n",
      "कि: 7\n",
      "ज: 7\n",
      "##द: 7\n",
      "##्: 7\n",
      "व: 7\n",
      "##े: 7\n",
      "##ोज: 7\n",
      "प: 6\n",
      "('आ', '##प'): 7\n",
      "('ख', '##ोज'): 7\n",
      "('है', '।'): 6\n",
      "('है', 'कि'): 4\n",
      "('हैं', ','): 4\n",
      "('##ोज', 'शब्द'): 4\n",
      "('##ों', 'की'): 3\n",
      "('##्', '##प'): 3\n",
      "('को', 'ब'): 3\n",
      "('##रल', '##ैंड'): 3\n",
      "('वि', '##के'): 3\n",
      "('ब', '##्रा'): 3\n",
      "('##्रा', '##यन'): 3\n",
      "('के', 'लिए'): 3\n",
      "('शब्द', '##ों'): 3\n",
      "('ज', '##ित'): 2\n",
      "('##ित', '##ना'): 2\n",
      "('##प', '##को'): 2\n",
      "('दिया', 'है'): 2\n",
      "('है', ','): 2\n",
      "('उ', '##तन'): 2\n",
      "('##तन', '##ा'): 2\n",
      "('##ा', 'ही'): 2\n",
      "('जा', '##ए'): 2\n",
      "('में', 'भी'): 2\n",
      "('ब', '##िल'): 2\n",
      "('भ', '##ाज'): 2\n",
      "('##ाज', '##पा'): 2\n",
      "('ने', '##ता'): 2\n",
      "('म', '##हा'): 2\n",
      "('#', '#'): 549\n",
      "('#', '्'): 51\n",
      "('#', 'ि'): 37\n",
      "('क', '#'): 34\n",
      "('्', '#'): 34\n",
      "('#', 'क'): 31\n",
      "('#', 'े'): 24\n",
      "('#', 'ो'): 20\n",
      "('म', 'ं'): 20\n",
      "('ं', '#'): 20\n",
      "('ि', '#'): 19\n",
      "('आ', '#'): 18\n",
      "('#', 'ा'): 18\n",
      "('ं', 'क'): 16\n",
      "('#', 'ी'): 13\n",
      "('#', 'न'): 12\n",
      "('े', '#'): 12\n",
      "('न', '#'): 11\n",
      "('ल', '#'): 11\n",
      "('व', '#'): 10\n",
      "('ा', '#'): 10\n",
      "('#', 'ं'): 10\n",
      "('ए', '#'): 10\n",
      "('स', '#'): 10\n",
      "('ह', '#'): 9\n",
      "('त', '#'): 9\n",
      "('#', 'ह'): 9\n",
      "('#', 'ै'): 9\n",
      "('ह', 'ं'): 9\n",
      "('#', 'ु'): 9\n",
      "('आ', 'क'): 31\n",
      "('क', 'ए'): 30\n",
      "('र', 'अ'): 25\n",
      "('अ', 'क'): 24\n",
      "('क', 'अ'): 23\n",
      "('ह', 'ऐ'): 23\n",
      "('य', 'आ'): 22\n",
      "('अ', 'न'): 21\n",
      "('अ', 'र'): 20\n",
      "('म', 'ए'): 20\n",
      "('ए', 'अं'): 20\n",
      "('न', 'ए'): 16\n",
      "('क', 'ओ'): 16\n",
      "('अ', 'ह'): 16\n",
      "('आ', 'ह'): 15\n",
      "('क', 'इ'): 14\n",
      "('क', 'आ'): 14\n",
      "('प', 'अ'): 14\n",
      "('न', 'आ'): 13\n",
      "('स', 'अ'): 12\n",
      "('इ', 'क'): 12\n",
      "('अ', 'त'): 12\n",
      "('क', 'ई'): 11\n",
      "('आ', 'य'): 11\n",
      "('ल', 'आ'): 10\n",
      "('र', 'आ'): 10\n",
      "('ए', 'क'): 9\n",
      "('आ', 'र'): 9\n",
      "('ज', 'अ'): 9\n",
      "('ह', 'अ'): 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('आ', 'क'): 31,\n",
       " ('क', 'ए'): 30,\n",
       " ('र', 'अ'): 25,\n",
       " ('अ', 'क'): 24,\n",
       " ('क', 'अ'): 23,\n",
       " ('ह', 'ऐ'): 23,\n",
       " ('य', 'आ'): 22,\n",
       " ('अ', 'न'): 21,\n",
       " ('अ', 'र'): 20,\n",
       " ('म', 'ए'): 20,\n",
       " ('ए', 'अं'): 20,\n",
       " ('न', 'ए'): 16,\n",
       " ('क', 'ओ'): 16,\n",
       " ('अ', 'ह'): 16,\n",
       " ('आ', 'ह'): 15,\n",
       " ('क', 'इ'): 14,\n",
       " ('क', 'आ'): 14,\n",
       " ('प', 'अ'): 14,\n",
       " ('न', 'आ'): 13,\n",
       " ('स', 'अ'): 12,\n",
       " ('इ', 'क'): 12,\n",
       " ('अ', 'त'): 12,\n",
       " ('क', 'ई'): 11,\n",
       " ('आ', 'य'): 11,\n",
       " ('ल', 'आ'): 10,\n",
       " ('र', 'आ'): 10,\n",
       " ('ए', 'क'): 9,\n",
       " ('आ', 'र'): 9,\n",
       " ('ज', 'अ'): 9,\n",
       " ('ह', 'अ'): 9}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mBertTokenizer = mBert()\n",
    "mB_encoding = mBertTokenizer.encode_plus(data, max_length=1000, truncation=True)\n",
    "mBert_tokens = mBertTokenizer.convert_ids_to_tokens(mB_encoding['input_ids'])\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = countF1(mBert_tokens)\n",
    "arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "model_save(model_name=\"mBert\",v_size=1000,arr=arr)\n",
    "topFreqUnigram(unigram_freqs,30)\n",
    "topFreqUnigram(bigram_freqs,30)\n",
    "topFreqUnigram(syllable_freqs,30)\n",
    "topFreqUnigram(char_freqs,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "682c6872-b0c9-4134-8ed1-35670f28280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "के: 22\n",
      "में: 20\n",
      "।: 18\n",
      "है: 15\n",
      "ब: 15\n",
      "##प: 14\n",
      "आ: 13\n",
      ",: 13\n",
      "को: 13\n",
      "##ी: 12\n",
      "##र: 12\n",
      "और: 12\n",
      "##ा: 11\n",
      "ख: 11\n",
      "##न: 10\n",
      "की: 9\n",
      "##स: 8\n",
      "म: 8\n",
      "स: 8\n",
      "का: 8\n",
      "हैं: 8\n",
      "कर: 7\n",
      "कि: 7\n",
      "ज: 7\n",
      "##द: 7\n",
      "##्: 7\n",
      "व: 7\n",
      "##े: 7\n",
      "##ोज: 7\n",
      "प: 6\n",
      "('आ', '##प'): 7\n",
      "('ख', '##ोज'): 7\n",
      "('है', '।'): 6\n",
      "('है', 'कि'): 4\n",
      "('हैं', ','): 4\n",
      "('##ोज', 'शब्द'): 4\n",
      "('##ों', 'की'): 3\n",
      "('##्', '##प'): 3\n",
      "('को', 'ब'): 3\n",
      "('##रल', '##ैंड'): 3\n",
      "('वि', '##के'): 3\n",
      "('ब', '##्रा'): 3\n",
      "('##्रा', '##यन'): 3\n",
      "('के', 'लिए'): 3\n",
      "('शब्द', '##ों'): 3\n",
      "('ज', '##ित'): 2\n",
      "('##ित', '##ना'): 2\n",
      "('##प', '##को'): 2\n",
      "('दिया', 'है'): 2\n",
      "('है', ','): 2\n",
      "('उ', '##तन'): 2\n",
      "('##तन', '##ा'): 2\n",
      "('##ा', 'ही'): 2\n",
      "('जा', '##ए'): 2\n",
      "('में', 'भी'): 2\n",
      "('ब', '##िल'): 2\n",
      "('भ', '##ाज'): 2\n",
      "('##ाज', '##पा'): 2\n",
      "('ने', '##ता'): 2\n",
      "('म', '##हा'): 2\n",
      "('#', '#'): 549\n",
      "('#', '्'): 51\n",
      "('#', 'ि'): 37\n",
      "('क', '#'): 34\n",
      "('्', '#'): 34\n",
      "('#', 'क'): 31\n",
      "('#', 'े'): 24\n",
      "('#', 'ो'): 20\n",
      "('म', 'ं'): 20\n",
      "('ं', '#'): 20\n",
      "('ि', '#'): 19\n",
      "('आ', '#'): 18\n",
      "('#', 'ा'): 18\n",
      "('ं', 'क'): 16\n",
      "('#', 'ी'): 13\n",
      "('#', 'न'): 12\n",
      "('े', '#'): 12\n",
      "('न', '#'): 11\n",
      "('ल', '#'): 11\n",
      "('व', '#'): 10\n",
      "('ा', '#'): 10\n",
      "('#', 'ं'): 10\n",
      "('ए', '#'): 10\n",
      "('स', '#'): 10\n",
      "('ह', '#'): 9\n",
      "('त', '#'): 9\n",
      "('#', 'ह'): 9\n",
      "('#', 'ै'): 9\n",
      "('ह', 'ं'): 9\n",
      "('#', 'ु'): 9\n",
      "('आ', 'क'): 31\n",
      "('क', 'ए'): 30\n",
      "('र', 'अ'): 25\n",
      "('अ', 'क'): 24\n",
      "('क', 'अ'): 23\n",
      "('ह', 'ऐ'): 23\n",
      "('य', 'आ'): 22\n",
      "('अ', 'न'): 21\n",
      "('अ', 'र'): 20\n",
      "('म', 'ए'): 20\n",
      "('ए', 'अं'): 20\n",
      "('न', 'ए'): 16\n",
      "('क', 'ओ'): 16\n",
      "('अ', 'ह'): 16\n",
      "('आ', 'ह'): 15\n",
      "('क', 'इ'): 14\n",
      "('क', 'आ'): 14\n",
      "('प', 'अ'): 14\n",
      "('न', 'आ'): 13\n",
      "('स', 'अ'): 12\n",
      "('इ', 'क'): 12\n",
      "('अ', 'त'): 12\n",
      "('क', 'ई'): 11\n",
      "('आ', 'य'): 11\n",
      "('ल', 'आ'): 10\n",
      "('र', 'आ'): 10\n",
      "('ए', 'क'): 9\n",
      "('आ', 'र'): 9\n",
      "('ज', 'अ'): 9\n",
      "('ह', 'अ'): 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('आ', 'क'): 31,\n",
       " ('क', 'ए'): 30,\n",
       " ('र', 'अ'): 25,\n",
       " ('अ', 'क'): 24,\n",
       " ('क', 'अ'): 23,\n",
       " ('ह', 'ऐ'): 23,\n",
       " ('य', 'आ'): 22,\n",
       " ('अ', 'न'): 21,\n",
       " ('अ', 'र'): 20,\n",
       " ('म', 'ए'): 20,\n",
       " ('ए', 'अं'): 20,\n",
       " ('न', 'ए'): 16,\n",
       " ('क', 'ओ'): 16,\n",
       " ('अ', 'ह'): 16,\n",
       " ('आ', 'ह'): 15,\n",
       " ('क', 'इ'): 14,\n",
       " ('क', 'आ'): 14,\n",
       " ('प', 'अ'): 14,\n",
       " ('न', 'आ'): 13,\n",
       " ('स', 'अ'): 12,\n",
       " ('इ', 'क'): 12,\n",
       " ('अ', 'त'): 12,\n",
       " ('क', 'ई'): 11,\n",
       " ('आ', 'य'): 11,\n",
       " ('ल', 'आ'): 10,\n",
       " ('र', 'आ'): 10,\n",
       " ('ए', 'क'): 9,\n",
       " ('आ', 'र'): 9,\n",
       " ('ज', 'अ'): 9,\n",
       " ('ह', 'अ'): 9}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mBertTokenizer_2000 = mBert()\n",
    "mB_encoding_2000 = mBertTokenizer.encode_plus(data, max_length=2000, truncation=True)\n",
    "mBert_tokens_2000 = mBertTokenizer_2000.convert_ids_to_tokens(mB_encoding_2000['input_ids'])\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = countF1(mBert_tokens)\n",
    "arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "model_save(model_name=\"mBert\",v_size=2000,arr=arr)\n",
    "topFreqUnigram(unigram_freqs,30)\n",
    "topFreqUnigram(bigram_freqs,30)\n",
    "topFreqUnigram(syllable_freqs,30)\n",
    "topFreqUnigram(char_freqs,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8471b3c-49c6-403b-b7f8-5d9fabd5d168",
   "metadata": {},
   "source": [
    "#### INDI_BERT 1000_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7dbea84-7774-4990-9b70-c013720305d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import collections\n",
    "def tokenizer_IndicBert():\n",
    "    tokenizer_indiBert = transformers.AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\",lang='hi')\n",
    "    return tokenizer_indiBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd03d18d-0a30-4dda-b9b2-09caf5d1dc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "के: 22\n",
      "में: 20\n",
      "।: 18\n",
      "है: 15\n",
      "ब: 15\n",
      "##प: 14\n",
      "आ: 13\n",
      ",: 13\n",
      "को: 13\n",
      "##ी: 12\n",
      "##र: 12\n",
      "और: 12\n",
      "##ा: 11\n",
      "ख: 11\n",
      "##न: 10\n",
      "की: 9\n",
      "##स: 8\n",
      "म: 8\n",
      "स: 8\n",
      "का: 8\n",
      "हैं: 8\n",
      "कर: 7\n",
      "कि: 7\n",
      "ज: 7\n",
      "##द: 7\n",
      "##्: 7\n",
      "व: 7\n",
      "##े: 7\n",
      "##ोज: 7\n",
      "प: 6\n",
      "('आ', '##प'): 7\n",
      "('ख', '##ोज'): 7\n",
      "('है', '।'): 6\n",
      "('है', 'कि'): 4\n",
      "('हैं', ','): 4\n",
      "('##ोज', 'शब्द'): 4\n",
      "('##ों', 'की'): 3\n",
      "('##्', '##प'): 3\n",
      "('को', 'ब'): 3\n",
      "('##रल', '##ैंड'): 3\n",
      "('वि', '##के'): 3\n",
      "('ब', '##्रा'): 3\n",
      "('##्रा', '##यन'): 3\n",
      "('के', 'लिए'): 3\n",
      "('शब्द', '##ों'): 3\n",
      "('ज', '##ित'): 2\n",
      "('##ित', '##ना'): 2\n",
      "('##प', '##को'): 2\n",
      "('दिया', 'है'): 2\n",
      "('है', ','): 2\n",
      "('उ', '##तन'): 2\n",
      "('##तन', '##ा'): 2\n",
      "('##ा', 'ही'): 2\n",
      "('जा', '##ए'): 2\n",
      "('में', 'भी'): 2\n",
      "('ब', '##िल'): 2\n",
      "('भ', '##ाज'): 2\n",
      "('##ाज', '##पा'): 2\n",
      "('ने', '##ता'): 2\n",
      "('म', '##हा'): 2\n",
      "('#', '#'): 549\n",
      "('#', '्'): 51\n",
      "('#', 'ि'): 37\n",
      "('क', '#'): 34\n",
      "('्', '#'): 34\n",
      "('#', 'क'): 31\n",
      "('#', 'े'): 24\n",
      "('#', 'ो'): 20\n",
      "('म', 'ं'): 20\n",
      "('ं', '#'): 20\n",
      "('ि', '#'): 19\n",
      "('आ', '#'): 18\n",
      "('#', 'ा'): 18\n",
      "('ं', 'क'): 16\n",
      "('#', 'ी'): 13\n",
      "('#', 'न'): 12\n",
      "('े', '#'): 12\n",
      "('न', '#'): 11\n",
      "('ल', '#'): 11\n",
      "('व', '#'): 10\n",
      "('ा', '#'): 10\n",
      "('#', 'ं'): 10\n",
      "('ए', '#'): 10\n",
      "('स', '#'): 10\n",
      "('ह', '#'): 9\n",
      "('त', '#'): 9\n",
      "('#', 'ह'): 9\n",
      "('#', 'ै'): 9\n",
      "('ह', 'ं'): 9\n",
      "('#', 'ु'): 9\n",
      "('आ', 'क'): 31\n",
      "('क', 'ए'): 30\n",
      "('र', 'अ'): 25\n",
      "('अ', 'क'): 24\n",
      "('क', 'अ'): 23\n",
      "('ह', 'ऐ'): 23\n",
      "('य', 'आ'): 22\n",
      "('अ', 'न'): 21\n",
      "('अ', 'र'): 20\n",
      "('म', 'ए'): 20\n",
      "('ए', 'अं'): 20\n",
      "('न', 'ए'): 16\n",
      "('क', 'ओ'): 16\n",
      "('अ', 'ह'): 16\n",
      "('आ', 'ह'): 15\n",
      "('क', 'इ'): 14\n",
      "('क', 'आ'): 14\n",
      "('प', 'अ'): 14\n",
      "('न', 'आ'): 13\n",
      "('स', 'अ'): 12\n",
      "('इ', 'क'): 12\n",
      "('अ', 'त'): 12\n",
      "('क', 'ई'): 11\n",
      "('आ', 'य'): 11\n",
      "('ल', 'आ'): 10\n",
      "('र', 'आ'): 10\n",
      "('ए', 'क'): 9\n",
      "('आ', 'र'): 9\n",
      "('ज', 'अ'): 9\n",
      "('ह', 'अ'): 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('आ', 'क'): 31,\n",
       " ('क', 'ए'): 30,\n",
       " ('र', 'अ'): 25,\n",
       " ('अ', 'क'): 24,\n",
       " ('क', 'अ'): 23,\n",
       " ('ह', 'ऐ'): 23,\n",
       " ('य', 'आ'): 22,\n",
       " ('अ', 'न'): 21,\n",
       " ('अ', 'र'): 20,\n",
       " ('म', 'ए'): 20,\n",
       " ('ए', 'अं'): 20,\n",
       " ('न', 'ए'): 16,\n",
       " ('क', 'ओ'): 16,\n",
       " ('अ', 'ह'): 16,\n",
       " ('आ', 'ह'): 15,\n",
       " ('क', 'इ'): 14,\n",
       " ('क', 'आ'): 14,\n",
       " ('प', 'अ'): 14,\n",
       " ('न', 'आ'): 13,\n",
       " ('स', 'अ'): 12,\n",
       " ('इ', 'क'): 12,\n",
       " ('अ', 'त'): 12,\n",
       " ('क', 'ई'): 11,\n",
       " ('आ', 'य'): 11,\n",
       " ('ल', 'आ'): 10,\n",
       " ('र', 'आ'): 10,\n",
       " ('ए', 'क'): 9,\n",
       " ('आ', 'र'): 9,\n",
       " ('ज', 'अ'): 9,\n",
       " ('ह', 'अ'): 9}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indi_tokenizer = tokenizer_IndicBert()\n",
    "indi_token = indi_tokenizer(data,max_length=1000, truncation=True)\n",
    "indi_tokens = indi_tokenizer.convert_ids_to_tokens(indi_token['input_ids'])\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = countF1(mBert_tokens)\n",
    "arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "model_save(model_name=\"IndicBert\",v_size=1000,arr=arr)\n",
    "topFreqUnigram(unigram_freqs,30)\n",
    "topFreqUnigram(bigram_freqs,30)\n",
    "topFreqUnigram(syllable_freqs,30)\n",
    "topFreqUnigram(char_freqs,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f87ef40-c0c9-45a7-af3e-41e757e82afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "के: 22\n",
      "में: 20\n",
      "।: 18\n",
      "है: 15\n",
      "ब: 15\n",
      "##प: 14\n",
      "आ: 13\n",
      ",: 13\n",
      "को: 13\n",
      "##ी: 12\n",
      "##र: 12\n",
      "और: 12\n",
      "##ा: 11\n",
      "ख: 11\n",
      "##न: 10\n",
      "की: 9\n",
      "##स: 8\n",
      "म: 8\n",
      "स: 8\n",
      "का: 8\n",
      "हैं: 8\n",
      "कर: 7\n",
      "कि: 7\n",
      "ज: 7\n",
      "##द: 7\n",
      "##्: 7\n",
      "व: 7\n",
      "##े: 7\n",
      "##ोज: 7\n",
      "प: 6\n",
      "('आ', '##प'): 7\n",
      "('ख', '##ोज'): 7\n",
      "('है', '।'): 6\n",
      "('है', 'कि'): 4\n",
      "('हैं', ','): 4\n",
      "('##ोज', 'शब्द'): 4\n",
      "('##ों', 'की'): 3\n",
      "('##्', '##प'): 3\n",
      "('को', 'ब'): 3\n",
      "('##रल', '##ैंड'): 3\n",
      "('वि', '##के'): 3\n",
      "('ब', '##्रा'): 3\n",
      "('##्रा', '##यन'): 3\n",
      "('के', 'लिए'): 3\n",
      "('शब्द', '##ों'): 3\n",
      "('ज', '##ित'): 2\n",
      "('##ित', '##ना'): 2\n",
      "('##प', '##को'): 2\n",
      "('दिया', 'है'): 2\n",
      "('है', ','): 2\n",
      "('उ', '##तन'): 2\n",
      "('##तन', '##ा'): 2\n",
      "('##ा', 'ही'): 2\n",
      "('जा', '##ए'): 2\n",
      "('में', 'भी'): 2\n",
      "('ब', '##िल'): 2\n",
      "('भ', '##ाज'): 2\n",
      "('##ाज', '##पा'): 2\n",
      "('ने', '##ता'): 2\n",
      "('म', '##हा'): 2\n",
      "('#', '#'): 549\n",
      "('#', '्'): 51\n",
      "('#', 'ि'): 37\n",
      "('क', '#'): 34\n",
      "('्', '#'): 34\n",
      "('#', 'क'): 31\n",
      "('#', 'े'): 24\n",
      "('#', 'ो'): 20\n",
      "('म', 'ं'): 20\n",
      "('ं', '#'): 20\n",
      "('ि', '#'): 19\n",
      "('आ', '#'): 18\n",
      "('#', 'ा'): 18\n",
      "('ं', 'क'): 16\n",
      "('#', 'ी'): 13\n",
      "('#', 'न'): 12\n",
      "('े', '#'): 12\n",
      "('न', '#'): 11\n",
      "('ल', '#'): 11\n",
      "('व', '#'): 10\n",
      "('ा', '#'): 10\n",
      "('#', 'ं'): 10\n",
      "('ए', '#'): 10\n",
      "('स', '#'): 10\n",
      "('ह', '#'): 9\n",
      "('त', '#'): 9\n",
      "('#', 'ह'): 9\n",
      "('#', 'ै'): 9\n",
      "('ह', 'ं'): 9\n",
      "('#', 'ु'): 9\n",
      "('आ', 'क'): 31\n",
      "('क', 'ए'): 30\n",
      "('र', 'अ'): 25\n",
      "('अ', 'क'): 24\n",
      "('क', 'अ'): 23\n",
      "('ह', 'ऐ'): 23\n",
      "('य', 'आ'): 22\n",
      "('अ', 'न'): 21\n",
      "('अ', 'र'): 20\n",
      "('म', 'ए'): 20\n",
      "('ए', 'अं'): 20\n",
      "('न', 'ए'): 16\n",
      "('क', 'ओ'): 16\n",
      "('अ', 'ह'): 16\n",
      "('आ', 'ह'): 15\n",
      "('क', 'इ'): 14\n",
      "('क', 'आ'): 14\n",
      "('प', 'अ'): 14\n",
      "('न', 'आ'): 13\n",
      "('स', 'अ'): 12\n",
      "('इ', 'क'): 12\n",
      "('अ', 'त'): 12\n",
      "('क', 'ई'): 11\n",
      "('आ', 'य'): 11\n",
      "('ल', 'आ'): 10\n",
      "('र', 'आ'): 10\n",
      "('ए', 'क'): 9\n",
      "('आ', 'र'): 9\n",
      "('ज', 'अ'): 9\n",
      "('ह', 'अ'): 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('आ', 'क'): 31,\n",
       " ('क', 'ए'): 30,\n",
       " ('र', 'अ'): 25,\n",
       " ('अ', 'क'): 24,\n",
       " ('क', 'अ'): 23,\n",
       " ('ह', 'ऐ'): 23,\n",
       " ('य', 'आ'): 22,\n",
       " ('अ', 'न'): 21,\n",
       " ('अ', 'र'): 20,\n",
       " ('म', 'ए'): 20,\n",
       " ('ए', 'अं'): 20,\n",
       " ('न', 'ए'): 16,\n",
       " ('क', 'ओ'): 16,\n",
       " ('अ', 'ह'): 16,\n",
       " ('आ', 'ह'): 15,\n",
       " ('क', 'इ'): 14,\n",
       " ('क', 'आ'): 14,\n",
       " ('प', 'अ'): 14,\n",
       " ('न', 'आ'): 13,\n",
       " ('स', 'अ'): 12,\n",
       " ('इ', 'क'): 12,\n",
       " ('अ', 'त'): 12,\n",
       " ('क', 'ई'): 11,\n",
       " ('आ', 'य'): 11,\n",
       " ('ल', 'आ'): 10,\n",
       " ('र', 'आ'): 10,\n",
       " ('ए', 'क'): 9,\n",
       " ('आ', 'र'): 9,\n",
       " ('ज', 'अ'): 9,\n",
       " ('ह', 'अ'): 9}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indi_tokenizer_2000 = tokenizer_IndicBert()\n",
    "indi_token_2000 = indi_tokenizer_2000(data,max_length=2000, truncation=True)\n",
    "indi_tokens_2000 = indi_tokenizer_2000.convert_ids_to_tokens(indi_token_2000['input_ids'])\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = countF1(mBert_tokens)\n",
    "arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "model_save(model_name=\"IndicBert\",v_size=2000,arr=arr)\n",
    "topFreqUnigram(unigram_freqs,30)\n",
    "topFreqUnigram(bigram_freqs,30)\n",
    "topFreqUnigram(syllable_freqs,30)\n",
    "topFreqUnigram(char_freqs,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4829d0b-1701-40d2-8374-a9d8ec8eee23",
   "metadata": {},
   "source": [
    "#### WhiteSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6adb290d-6b14-4dad-812d-3bb67dce2e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "के: 316314\n",
      "में: 237426\n",
      "की: 189769\n",
      "को: 145266\n",
      "से: 136979\n",
      "और: 114563\n",
      "का: 109743\n",
      "ने: 102378\n",
      "पर: 88560\n",
      "है।: 88260\n",
      "कि: 76741\n",
      "है: 72656\n",
      "भी: 63920\n",
      "एक: 49279\n",
      "लिए: 47955\n",
      "इस: 47558\n",
      "कर: 44563\n",
      "नहीं: 44168\n",
      "ही: 40104\n",
      "तो: 33469\n",
      "हैं।: 33204\n",
      "हो: 32014\n",
      "यह: 30837\n",
      "करने: 29399\n",
      "किया: 28818\n",
      "साथ: 25201\n",
      "है.: 24954\n",
      "कहा: 24141\n",
      "हैं: 23759\n",
      "बाद: 22786\n",
      "('के', 'लिए'): 42448\n",
      "('है', 'कि'): 24816\n",
      "('के', 'साथ'): 17075\n",
      "('कहा', 'कि'): 15922\n",
      "('के', 'बाद'): 14163\n",
      "('है', 'और'): 10096\n",
      "('ने', 'कहा'): 9098\n",
      "('करने', 'के'): 8826\n",
      "('बताया', 'कि'): 6392\n",
      "('को', 'लेकर'): 5977\n",
      "('गया', 'है।'): 5678\n",
      "('.', '.'): 5638\n",
      "('रहा', 'है।'): 5483\n",
      "('के', 'खिलाफ'): 5312\n",
      "('के', 'दौरान'): 5162\n",
      "('के', 'बीच'): 5117\n",
      "('बारे', 'में'): 5059\n",
      "('करते', 'हुए'): 4817\n",
      "('रहे', 'हैं।'): 4701\n",
      "('में', 'भी'): 4682\n",
      "('कर', 'रहे'): 4637\n",
      "('जा', 'रहा'): 4633\n",
      "('हैं', 'और'): 4581\n",
      "('उन्होंने', 'कहा'): 4578\n",
      "('रही', 'है।'): 4560\n",
      "('किया', 'गया'): 4522\n",
      "('में', 'एक'): 4442\n",
      "('ने', 'बताया'): 4316\n",
      "('करने', 'की'): 4280\n",
      "('साथ', 'ही'): 4259\n",
      "('म', 'ं'): 271255\n",
      "('ं', 'क'): 184372\n",
      "('ह', 'ं'): 126522\n",
      "('क', 'स'): 109113\n",
      "('ह', '।'): 93090\n",
      "('क', 'ल'): 84980\n",
      "('र', 'क'): 81905\n",
      "('क', 'र'): 80472\n",
      "('क', 'क'): 75385\n",
      "('प', 'र'): 74843\n",
      "('न', 'क'): 74018\n",
      "('क', 'य'): 64340\n",
      "('क', 'म'): 64239\n",
      "('ल', 'क'): 61135\n",
      "('य', 'ं'): 58226\n",
      "('स', 'क'): 57325\n",
      "('त', 'ह'): 56723\n",
      "('क', 'प'): 55192\n",
      "('ल', 'ए'): 54306\n",
      "('ं', 'स'): 52955\n",
      "('क', 'ब'): 51616\n",
      "('ं', 'म'): 50605\n",
      "('ं', 'न'): 50168\n",
      "('त', 'र'): 48550\n",
      "('क', 'अ'): 48401\n",
      "('र', 'म'): 47798\n",
      "('नह', 'ं'): 47406\n",
      "('ं', '।'): 46163\n",
      "('न', 'ह'): 44154\n",
      "('य', 'क'): 44139\n",
      "('र', 'अ'): 501180\n",
      "('अ', 'क'): 429218\n",
      "('क', 'अ'): 422675\n",
      "('अ', 'र'): 408662\n",
      "('क', 'ए'): 407129\n",
      "('अ', 'ह'): 374151\n",
      "('प', 'अ'): 358976\n",
      "('अ', 'न'): 358475\n",
      "('स', 'अ'): 350066\n",
      "('ए', 'अं'): 341741\n",
      "('आ', 'क'): 336575\n",
      "('न', 'ए'): 328953\n",
      "('क', 'आ'): 314321\n",
      "('य', 'आ'): 297771\n",
      "('ह', 'ऐ'): 297199\n",
      "('म', 'ए'): 296594\n",
      "('ए', 'क'): 270785\n",
      "('अ', 'म'): 250370\n",
      "('अ', 'त'): 243052\n",
      "('क', 'ई'): 236906\n",
      "('आ', 'र'): 230627\n",
      "('न', 'अ'): 226336\n",
      "('आ', 'ह'): 224131\n",
      "('स', 'ए'): 216056\n",
      "('ओ', 'अं'): 212156\n",
      "('अं', 'क'): 207159\n",
      "('क', 'ओ'): 204712\n",
      "('क', 'इ'): 201166\n",
      "('आ', 'न'): 200173\n",
      "('र', 'आ'): 199968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('र', 'अ'): 501180,\n",
       " ('अ', 'क'): 429218,\n",
       " ('क', 'अ'): 422675,\n",
       " ('अ', 'र'): 408662,\n",
       " ('क', 'ए'): 407129,\n",
       " ('अ', 'ह'): 374151,\n",
       " ('प', 'अ'): 358976,\n",
       " ('अ', 'न'): 358475,\n",
       " ('स', 'अ'): 350066,\n",
       " ('ए', 'अं'): 341741,\n",
       " ('आ', 'क'): 336575,\n",
       " ('न', 'ए'): 328953,\n",
       " ('क', 'आ'): 314321,\n",
       " ('य', 'आ'): 297771,\n",
       " ('ह', 'ऐ'): 297199,\n",
       " ('म', 'ए'): 296594,\n",
       " ('ए', 'क'): 270785,\n",
       " ('अ', 'म'): 250370,\n",
       " ('अ', 'त'): 243052,\n",
       " ('क', 'ई'): 236906,\n",
       " ('आ', 'र'): 230627,\n",
       " ('न', 'अ'): 226336,\n",
       " ('आ', 'ह'): 224131,\n",
       " ('स', 'ए'): 216056,\n",
       " ('ओ', 'अं'): 212156,\n",
       " ('अं', 'क'): 207159,\n",
       " ('क', 'ओ'): 204712,\n",
       " ('क', 'इ'): 201166,\n",
       " ('आ', 'न'): 200173,\n",
       " ('र', 'आ'): 199968}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_data = data.strip()\n",
    "if not strip_data:\n",
    "    white_space_token=[]\n",
    "else :\n",
    "    white_space_token = data.split()\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = countF1(white_space_token)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "topFreqUnigram(unigram_freqs,30)\n",
    "topFreqUnigram(bigram_freqs,30)\n",
    "topFreqUnigram(syllable_freqs,30)\n",
    "topFreqUnigram(char_freqs,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b970690-ae13-46fb-b174-1ebe75621285",
   "metadata": {},
   "source": [
    "## Q5 Calculating precision, recall and F1 score using answers of Q3 and outputs of tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5731d402-6326-4ce8-abc9-106736e602b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true_tokens,predicted_tokens):\n",
    "    true_positives = len(set(predicted_tokens).intersection(set(true_tokens)))\n",
    "    false_positives = len(set(predicted_tokens).difference(set(true_tokens)))\n",
    "    return true_positives/(true_positives+false_positives)\n",
    "\n",
    "def recall(true_tokens,predicted_tokens):\n",
    "    true_positives = len(set(predicted_tokens).intersection(set(true_tokens)))\n",
    "    false_negatives = len(set(true_tokens).difference(set(predicted_tokens)))\n",
    "    return true_positives/(true_positives+false_negatives)\n",
    "\n",
    "def f1_score(true_tokens,predicted_tokens):\n",
    "    p = precision(true_tokens,predicted_tokens)\n",
    "    r = recall(true_tokens,predicted_tokens)\n",
    "    return 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30b14536-63bd-45b0-91a3-f5863823ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "उसकी हालत में सुधार हो रहा है।अज नित नए मीडिया संसथान खुल रहे है, जिसकी फीस हजारों रुपये है। प्रोफेसनल कोर्स होने के चलते छात्र इसमे प्रवेश लेते है। लाखों रुपये खर्च कर पढ़ाई/ प्रशिक्षण प्राप्त करते है। लेकिन उनके सपने तब टूट जाते है जब मीडिया संस्थानों में पहले से जमे दिग्गज ही उनका आर्थिक और मानसिक शोषण शुरू कर देते है।यह आरोप उनपर अंपायर जॉन वार्ड तथा शॉन क्रेग ने लगाए थे।फिर धीरे से गांड को खोल के अपना सुपाड़ा अन्दर कर दिया. सिखा को बड़ा दर्द हुआ और वो कराह उठी.मैंने उसके शोल्डर के ऊपर किस कर दी. और लंड को ऐसे उतना ही अन्दर रहने दिया.ई-गवर्नेंस के तहत मोबाइल फोन जैसे उपकरणों से भी सरकारी सेवाओं तक पहुंचा जा सकता है।बुधवार को शिमला का तापमान 1.4 डिग्री सेल्सियस दर्ज किया गया, जबकि पर्यटन स्थल मनाली में यह शून्य से 2.6 डिग्री सेल्सियस नीचे दर्ज किया गया।उनका स्वास्थ्य गंभीर रूप से प्रभावित था।अभी लीगली बटवारा नहीं हुआ है। सहमति से तलाक़ होने पर मैं अपने पति से अपने लिए या मुझे ना भी मिले तो अपने बेटे के लिए उक्त संपत्ति में हिस्सा माँग सकती हूँ। अगर हाँ तो उस का निर्धारण किस प्रकार से होगा?इसकी वजह बर्फ़ पिघलने की गति बढ़ी है. मनोहर अब तक सबसे लोकप्रिय सीएम : बराला पर इस बात को भी दरकिनार नहीं किया जा सकता है कि इन फिल्मों की असफलता का सबसे ज़्यादा नुक़सान उन्हें ही हुआ. पीसीओएस आम समस्या है और यह 12 साल की बच्चियों से लेकर 45 साल की महिलाओं में किसी को भी हो सकती है। आनी  — आनी खंड के कराणा पंचायत के जाबो क्षेत्र के लोग पिछले करीब एक हफ्ते से पैदल अपने घरों को जाने के लिए मजबूर हैं। पूर्वोत्तर राज्यों में विरोध-प्रदर्शन कर रहे लोग आरएसएस गो-बैक के नारे लगा रहे हैं, साथ ही अपने नारों में सत्ताधारी भाजपा को सतर्क कर रहे हैं. loading. . . सड़कों पर उतरे ऑल असम स्टूडेंट्स यूनियन के कार्यकर्ताओं ने इससे पहले नगरिकता बिल के ख़िलाफ़ मशाल जुलूस निकालकर अपना विरोध जताया. रमन सिंह पर फूल बरसाने को नहीं मिल रहे आमजन, शिक्षकों को जारी हुआ सीएम पर फूल बरसाने का लिखित आदेश प्रथम दो माह तक कैंसर के रोगी का आहार सिर्फ अंगूर ही होना चाहिए। हालांकि, 27 जनवरी को पटना पुलिस की एक टीम से सेना में भर्ती की परीक्षा का पर्चा  लीक करने का दावा करने वाले अपराधियों के एक गिरोह के ठिकाने पर छापा मारा था। इस समय वह नस्लीय आधार पर भेदभाव करती नजर आई और उसका व्यवहार खासा रूखा रहा। वह भी बिना अधिक रन दिए। युधिष्ठिर की उदारता भी अलौकिक थी । जब कौरवो ने किसी प्रकार भी इनका राज्य लौटाना मंजूर नहीं किया तो इन्होंने केवल पांच गांव लेकर संतोष करना स्वीकार कर लिया और भगवान् श्रीकृष्णके द्वारा दुर्योधन को यह कहला भेजा कि यदि वह हमें हमारे इच्छानुसार केवल पांच गांव देना मंजूर कर ले तो हम युद्ध न करे । वरुण गाँधी ने जाति धर्म की राजनीती पर चोट की। इस बात की जानकारी फ्लॉरेडा के शेरिफ ऑफिस ने बताई. अमेरिका की संसद ने भारत की आपत्ति के बावजूद लोकप्रिय एच-1बी तथा एल-1 वीजा पर विशेष शुल्क दोगुना कर 4,500 डॉलर तक कर दिया है। उपायुक्त मंडी ऋग्वेद ठाकुर की सोच से शुरू किया गया संबल कार्यक्रम पूरे प्रदेश में सिर्फ मंडी जिला में चला हुआ है। सवाल 6. रोनाल्डो लाईतोंजम किस खेल से जुड़े हुए हैं?\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\n",
    "file = 'CS689.txt' \n",
    "\n",
    "with open(file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line !=\"\\n\":\n",
    "            temp = line.strip()\n",
    "            txt += temp[3:]\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7b7a49e-1176-4b03-a583-1ecec866da86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All true tokens: ['उसकी', 'हालत में', 'सुधार', 'हो रहा है।अज', 'नित', 'नए', 'मीडिया संसथान', 'खुल', 'रहे है', 'जिसकी', 'फीस', 'हजारों रुपये', 'है। प्रोफेसनल कोर्स', 'होने के', 'चलते', 'छात्र', 'इसमे', 'प्रवेश', 'लेते है।', 'लाखों रुपये', 'खर्च कर', 'पढ़ाई/ प्रशिक्षण', 'प्राप्त', 'करते है।', 'लेकिन', 'उनके', 'सपने', 'तब', 'टूट', 'जाते है', 'जब', 'मीडिया संस्थानों में', 'पहले से', 'जमे', 'दिग्गज ही', 'उनका', 'आर्थिक', 'और', 'मानसिक', 'शोषण', 'शुरू', 'कर', 'देते है।यह', 'आरोप', 'उनपर', 'अंपायर जॉन वार्ड', 'तथा', 'शॉन क्रेग ने', 'लगाए थे।फिर', 'धीरे से', 'गांड को', 'खोल के', 'अपना', 'सुपाड़ा', 'अन्दर', 'कर दिया.', 'सिखा को', 'बड़ा', 'दर्द', 'हुआ', 'और', 'वो', 'कराह', 'उठी.', 'मैंने', 'उसके', 'शोल्डर के', 'ऊपर', 'किस', 'कर दी.', 'और', 'लंड को', 'ऐसे', 'उतना ही', 'अन्दर', 'रहने दिया.ई-गवर्नेंस के', 'तहत', 'मोबाइल फोन', 'जैसे', 'उपकरणों', 'से भी', 'सरकारी सेवाओं', 'तक', 'पहुंचा', 'जा सकता है।बुधवार को', 'शिमला का', 'तापमान', '1.4 डिग्री सेल्सियस', 'दर्ज', 'किया गया', 'जबकि', 'पर्यटन स्थल', 'मनाली में', 'यह', 'शून्य से', '2.6 डिग्री सेल्सियस', 'नीचे', 'दर्ज', 'किया गया।उनका', 'स्वास्थ्य', 'गंभीर', 'रूप से', 'प्रभावित', 'था।अभी', 'लीगली', 'बटवारा', 'नहीं', 'हुआ है।', 'सहमति से', 'तलाक़', 'होने पर', 'मैं', 'अपने', 'पति से', 'अपने लिए', 'या', 'मुझे', 'ना', 'भी', 'मिले', 'तो', 'अपने', 'बेटे के लिए', 'उक्त', 'संपत्ति में', 'हिस्सा', 'माँग', 'सकती हूँ।', 'अगर', 'हाँ', 'तो', 'उस का', 'निर्धारण', 'किस', 'प्रकार से', 'होगा?इसकी', 'वजह', 'बर्फ़', 'पिघलने की', 'गति', 'बढ़ी है.मनोहर', 'अब', 'तक', 'सबसे', 'लोकप्रिय', 'सीएम :', 'बराला पर', 'इस', 'बात को भी', 'दरकिनार', 'नहीं', 'किया', 'जा', 'सकता है', 'कि', 'इन', 'फिल्मों की', 'असफलता का', 'सबसे', 'ज़्यादा', 'नुक़सान', 'उन्हें ही', 'हुआ.पीसीओएस', 'आम', 'समस्या है', 'और', 'यह', '12 साल की', 'बच्चियों से', 'लेकर', '45 साल की', 'महिलाओं में', 'किसी', 'को भी', 'हो', 'सकती है।आनी  — आनी', 'खंड के', 'कराणा', 'पंचायत के', 'जाबो क्षेत्र के', 'लोग', 'पिछले', 'करीब', 'एक हफ्ते से', 'पैदल', 'अपने', 'घरों को', 'जाने के लिए', 'मजबूर हैं।पूर्वोत्तर राज्यों में', 'विरोध-प्रदर्शन', 'कर रहे', 'लोग', 'आरएसएस गो-बैक के', 'नारे', 'लगा रहे हैं', 'साथ ही', 'अपने', 'नारों में', 'सत्ताधारी', 'भाजपा को', 'सतर्क', 'कर रहे हैं', '. सड़कों पर', 'उतरे', 'ऑल असम स्टूडेंट्स यूनियन के', 'कार्यकर्ताओं ने', 'इससे', 'पहले', 'नगरिकता बिल के', 'ख़िलाफ़', 'मशाल जुलूस', 'निकालकर', 'अपना', 'विरोध', 'जताया.रमन सिंह पर', 'फूल', 'बरसाने को', 'नहीं', 'मिल रहे', 'आमजन', 'शिक्षकों को', 'जारी हुआ', 'सीएम पर', 'फूल', 'बरसाने का', 'लिखित', 'आदेशप्रथम', 'दो माह', 'तक', 'कैंसर के', 'रोगी का', 'आहार', 'सिर्फ', 'अंगूर ही', 'होना चाहिए।हालांकि', '27 जनवरी को', 'पटना पुलिस की', 'एक', 'टीम से', 'सेना में', 'भर्ती की', 'परीक्षा का', 'पर्चा', 'लीक', 'करने का', 'दावा', 'करने', 'वाले', 'अपराधियों के', 'एक गिरोह के', 'ठिकाने पर', 'छापा', 'मारा था।इस', 'समय', 'वह', 'नस्लीय', 'आधार पर', 'भेदभाव', 'करती', 'नजर', 'आई', 'और', 'उसका', 'व्यवहार', 'खासा', 'रूखा रहा।वह भी', 'बिना', 'अधिक', 'रन', 'दिए।युधिष्ठिर की', 'उदारता भी', 'अलौकिक थी ।', 'जब', 'कौरवो ने', 'किसी', 'प्रकार भी', 'इनका', 'राज्य', 'लौटाना', 'मंजूर', 'नहीं', 'किया', 'तो', 'इन्होंने', 'केवल', 'पांच गांव', 'लेकर', 'संतोष', 'करना', 'स्वीकार', 'कर लिया', 'और', 'भगवान्', 'श्रीकृष्णके द्वारा', 'दुर्योधन को', 'यह', 'कहला', 'भेजा', 'कि', 'यदि', 'वह', 'हमें', 'हमारे', 'इच्छानुसार', 'केवल', 'पांच गांव', 'देना', 'मंजूर', 'कर ले', 'तो', 'हम', 'युद्ध', 'न', 'करेवरुण गाँधी ने', 'जाति धर्म की', 'राजनीती पर', 'चोट कीइस बात की', 'जानकारी', 'फ्लॉरेडा के', 'शेरिफ ऑफिस ने', 'बताई.अमेरिका की', 'संसद ने', 'भारत की', 'आपत्ति के', 'बावजूद', 'लोकप्रिय', 'एच-1बी', 'तथा', 'एल-1', 'वीजा पर', 'विशेष', 'शुल्क', 'दोगुना कर', '4', '500 डॉलर', 'तक', 'कर दिया है।उपायुक्त मंडी ऋग्वेद ठाकुर की', 'सोच से', 'शुरू', 'किया गया', 'संबल', 'कार्यक्रम', 'पूरे', 'प्रदेश में', 'सिर्फ', 'मंडी जिला में', 'चला', 'हुआ है।सवाल 6.', 'रोनाल्डो लाईतोंजम', 'किस', 'खेल से', 'जुड़े', 'हुए हैं?']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'CS689_answer.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    tokens = file.read().split(',')\n",
    "true_tokens = [word.strip() for word in tokens]\n",
    "true_tokens = [word.replace('\\n', '') for word in true_tokens]\n",
    "print(\"All true tokens:\", true_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c6ccaf-9fb8-49d9-a1f4-3c67cc3b047f",
   "metadata": {},
   "source": [
    "#### Unigram_1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85c54b89-3f26-421f-8611-52e6d86cb68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.22\n",
      "Recall: 0.25\n",
      "F1-Score: 0.23\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pred_tokens = unigram_1000_tokenizer.encode(txt, out_type=str)\n",
    "pred_tokens = [tok[1:] if tok.startswith('▁') else tok for tok in pred_tokens]\n",
    "# Precision\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8db6a-4531-454a-9882-e2453d061837",
   "metadata": {},
   "source": [
    "#### Unigram_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a958d8a7-adb8-4f81-b496-59a5be05b218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.24\n",
      "Recall: 0.32\n",
      "F1-Score: 0.27\n"
     ]
    }
   ],
   "source": [
    "pred_tokens = unigram_2000_tokenizer.encode(txt, out_type=str)\n",
    "# Precision\n",
    "pred_tokens = [tok[1:] if tok.startswith('▁') else tok for tok in pred_tokens]\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3f669-e302-4834-aaf0-ceddc5bf4290",
   "metadata": {},
   "source": [
    "#### BPE_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "092d8989-ffcf-4e9a-a276-36a6d8ce88c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.14\n",
      "Recall: 0.18\n",
      "F1-Score: 0.16\n"
     ]
    }
   ],
   "source": [
    "pred_tokens = bpe_1000_tokenizer.encode(txt, out_type=str)\n",
    "pred_tokens = [tok[1:] if tok.startswith('▁') else tok for tok in pred_tokens]\n",
    "\n",
    "# Precision\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd0c6ad-9408-41db-90ab-0b4030588175",
   "metadata": {},
   "source": [
    "#### BPE_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e7c0411-0a29-4a80-a4fb-dbaed10e20d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.19\n",
      "Recall: 0.29\n",
      "F1-Score: 0.23\n"
     ]
    }
   ],
   "source": [
    "pred_tokens = bpe_2000_tokenizer.encode(txt, out_type=str)\n",
    "pred_tokens = [tok[1:] if tok.startswith('▁') else tok for tok in pred_tokens]\n",
    "# Precision\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03e64a-31ea-4209-905b-e4832da2fc51",
   "metadata": {},
   "source": [
    "#### Mbert_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "028bef87-1009-436a-9c5c-488b953bc2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.16\n",
      "Recall: 0.21\n",
      "F1-Score: 0.18\n"
     ]
    }
   ],
   "source": [
    "mBtokenizer = mBert()\n",
    "mB_enc = mBtokenizer.encode_plus(txt, max_length=1000, truncation=True)\n",
    "pred_tokens = mBertTokenizer.convert_ids_to_tokens(mB_enc['input_ids'])\n",
    "pred_tokens = [tok[1:] if tok.startswith('▁') else tok for tok in pred_tokens]\n",
    "\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79468fc4-5264-4806-982e-4fdbe310980c",
   "metadata": {},
   "source": [
    "#### Mbert_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a729ef7-37d5-4e0c-8098-d2aa3001d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.15\n",
      "Recall: 0.22\n",
      "F1-Score: 0.18\n"
     ]
    }
   ],
   "source": [
    "mBtokenizer = mBert()\n",
    "mB_enc = mBtokenizer.encode_plus(txt, max_length=2000, truncation=True)\n",
    "pred_tokens = mBertTokenizer.convert_ids_to_tokens(mB_enc['input_ids'])\n",
    "pred_tokens = [tok[1:] if tok.startswith('▁') else tok for tok in pred_tokens]\n",
    "\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e2f17-909d-4623-bc6a-120ff6a05555",
   "metadata": {},
   "source": [
    "#### Indi_Bert 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3bc0df72-bdfa-426a-842d-aade1553be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.09\n",
      "Recall: 0.08\n",
      "F1-Score: 0.09\n"
     ]
    }
   ],
   "source": [
    "indi_teast_token = tokenizer_IndicBert()\n",
    "indi_test_token = indi_teast_token(txt,max_length=1000, truncation=True)\n",
    "pred_tokens = indi_teast_token.convert_ids_to_tokens(indi_test_token['input_ids'])\n",
    "pred_tokens = [tok[1:] if tok.startswith('▁') else tok for tok in pred_tokens]\n",
    "\n",
    "\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d29088-563e-45c5-84cb-83ab249811ad",
   "metadata": {},
   "source": [
    "#### Indi_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6aba5832-d5ae-4a11-b294-bd95d57d3292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.09\n",
      "Recall: 0.08\n",
      "F1-Score: 0.09\n"
     ]
    }
   ],
   "source": [
    "indi_teast_token = tokenizer_IndicBert()\n",
    "indi_test_token = indi_teast_token(txt,max_length=2000, truncation=True)\n",
    "pred_tokens = indi_teast_token.convert_ids_to_tokens(indi_test_token['input_ids'])\n",
    "pred_tokens = [tok[1:] if tok.startswith('▁') else tok for tok in pred_tokens]\n",
    "\n",
    "\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e23657-0949-4e7f-9a74-a7ad49ceb262",
   "metadata": {},
   "source": [
    "#### WhiteSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07f66d27-00be-441c-8231-f262268c0c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.43\n",
      "Recall: 0.52\n",
      "F1-Score: 0.47\n"
     ]
    }
   ],
   "source": [
    "strip_data = txt.strip()\n",
    "if not strip_data:\n",
    "    pred_tokens=[]\n",
    "else :\n",
    "    pred_tokens = txt.split()\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2fd22-18ec-4f42-9ad9-6b00446aa81e",
   "metadata": {},
   "source": [
    "## Q6.\n",
    "### After finding the tokens in Q3 and comparing them with all the tokenizers with different vocab sizes, the results are as follows:\n",
    "#### order -> unigram_1000 ->unigram_2000 -> bpe_1000 -> bpe_2000 -> mBert_1000 -> mBert_2000 -> Indi_1000 ->Indi_2000 ->whitespace\n",
    "\n",
    "#### Precision = 0.22 -> 0.24 -> 0.14 -> 0.19 -> 0.16 -> 0.15 -> 0.09 -> 0.09 -> 0.43\n",
    "#### Recall    = 0.25 -> 0.32 -> 0.18 -> 0.29 -> 0.21 -> 0.22 -> 0.08 -> 0.08 -> 0.52\n",
    "#### F1        = 0.23 -> 0.27 -> 0.16 -> 0.23 -> 0.18 -> 0.18 -> 0.09 -> 0.09 -> 0.47\n",
    "\n",
    "#### These models have varying results but have room for improvement. White space tokenizer performs well because the truth value is given by humans as it makes tokens because of whitespace. Indic Bert has the lowest values among all of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac97ba1-8ceb-4e7e-abc8-02da8e83d526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
